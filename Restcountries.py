ì½˜í…ì¸ ë¡œ ê±´ë„ˆë›°ê¸°
ë‚˜ì˜ ë§:
ë„ì»¤ ì´ë¯¸ì§€ ì„¤ì¹˜ì¤‘ 
C:\Users\ryanp\OneDrive\ë°”íƒ• í™”ë©´\kdt\airflow-setup>curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.9.1/docker-compose.yaml'
curl: Failed to extract a filename from the URL to use for storage
curl: (3) URL using bad/illegal format or missing URL
ë¼ê³ ëœ¨ëŠ”ë° ë¬´ìŠ¨ ì˜¤ë¥˜ì•¼
ChatGPTì˜ ë§:
ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë³´ë©´ ë‘ ê°€ì§€ ë¬¸ì œë¥¼ ë§í•˜ê³  ìˆì–´:

ğŸ§¨ ì˜¤ë¥˜ ì›ì¸
1. curl: Failed to extract a filename from the URL to use for storage
-O ì˜µì…˜ì€ ì„œë²„ë¡œë¶€í„° ë°›ì€ íŒŒì¼ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì €ì¥í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì¸ë°, ì´ë•Œ URLì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì´ ì•„ë‹ˆë©´ curlì´ ì €ì¥í•  íŒŒì¼ëª…ì„ ì¶”ì¶œí•˜ì§€ ëª»í•¨.

-LfO ì¤‘ -Oë¥¼ ì“°ë ¤ë©´ URLì´ ë°˜ë“œì‹œ ìœ íš¨í•œ íŒŒì¼ ê²½ë¡œì—¬ì•¼ í•˜ê³ , ëì— íŒŒì¼ëª…ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•¨.

2. curl: (3) URL using bad/illegal format or missing URL
ì´ê±´ URL ìì²´ì— ë¬¸ì œê°€ ìˆê±°ë‚˜, curl ëª…ë ¹ì–´ ì‘ì„± ì‹œ ë¬¸ë²•ì´ ì˜ëª»ë˜ì—ˆìŒì„ ëœ»í•¨.

URLì„ ' (ì‘ì€ ë”°ì˜´í‘œ)ë¡œ ê°ìŒŒëŠ”ë° Windows ëª…ë ¹ í”„ë¡¬í”„íŠ¸(cmd)ì—ì„œëŠ” 'ì´ ì¸ì‹ë˜ì§€ ì•Šê³ , " (í°ë”°ì˜´í‘œ)ë¥¼ ì¨ì•¼ í•¨.

âœ… í•´ê²° ë°©ë²•
ì•„ë˜ì²˜ëŸ¼ í°ë”°ì˜´í‘œë¡œ ë°”ê¾¸ê³  ëª…ë ¹ì–´ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ë´:

curl -LfO "https://airflow.apache.org/docs/apache-airflow/2.9.1/docker-compose.yaml"
ë˜ëŠ”, ì €ì¥í•  íŒŒì¼ëª…ì„ ì§€ì •í•˜ê³  ì‹¶ë‹¤ë©´ -o ì˜µì…˜ì„ ì¨ë„ ë¨:

curl -L "https://airflow.apache.org/docs/apache-airflow/2.9.1/docker-compose.yaml" -o docker-compose.yaml
ğŸ’¡ ì°¸ê³ 
-L: ë¦¬ë””ë ‰ì…˜ì´ ìˆì„ ê²½ìš° ë”°ë¼ê°

-f: ìš”ì²­ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë°˜í™˜

-O: URLì—ì„œ íŒŒì¼ ì´ë¦„ ì¶”ì¶œí•´ ê·¸ëŒ€ë¡œ ì €ì¥

-o íŒŒì¼ëª…: ì§ì ‘ ì €ì¥ íŒŒì¼ëª…ì„ ì§€ì •

ê¶ê¸ˆí•œ ê±° ë” ìˆìœ¼ë©´ ì´ì–´ì„œ ë§í•´ì¤˜.

ë‚˜ì˜ ë§:
HTTP code 500) server error - Ports are not available: exposing port TCP 0.0.0.0:8080 -> 127.0.0.1:0: listen tcp 0.0.0.0:8080: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
ì´ê±´ ë¬´ìŠ¨ì˜¤ë¥˜ì•¼ ë„ì»¤ì— ì´ë¯¸ì§€ ì„¤ì¹˜ë¥¼ ë‹¤í•´ì„œ 8080 ì ‘ì†í•´ë³´ë ¤ê³ í•˜ë‹ˆ ì´ëŸ¬ë„¤
ChatGPTì˜ ë§:
ì´ ì˜¤ë¥˜ëŠ” ëª…í™•í•˜ê²Œ ë§í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì´ì•¼:

ğŸ§¨ ì˜¤ë¥˜ ì›ì¸
Ports are not available: exposing port TCP 0.0.0.0:8080
-> 127.0.0.1:0: listen tcp 0.0.0.0:8080: bind:
Only one usage of each socket address (protocol/network address/port) is normally permitted.
âœ… í•´ì„
ë„ì»¤ ì»¨í…Œì´ë„ˆê°€ 8080ë²ˆ í¬íŠ¸ë¥¼ ì‚¬ìš©í•˜ë ¤ê³  í–ˆëŠ”ë°,
ì´ë¯¸ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ ê·¸ í¬íŠ¸ë¥¼ ì‚¬ìš© ì¤‘ì´ë¼ ë°”ì¸ë”©(bind) ì‹¤íŒ¨í•œ ê±°ì•¼.

Windowsì—ì„œëŠ” ê°™ì€ í¬íŠ¸ë¥¼ ì—¬ëŸ¬ ì•±ì´ ë™ì‹œì— ì‚¬ìš©í•˜ëŠ” ê²Œ ì›ì¹™ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•´ì„œ ì´ëŸ° ì—ëŸ¬ê°€ ë°œìƒí•´.

ğŸ” í•´ê²° ë°©ë²•
1. ğŸ” í¬íŠ¸ 8080ì„ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ì°¾ê¸°
netstat -ano | findstr :8080
ê²°ê³¼ ì˜ˆì‹œ:

TCP    0.0.0.0:8080           0.0.0.0:0              LISTENING       1234
ë§ˆì§€ë§‰ì— ë‚˜ì˜¤ëŠ” ìˆ«ì 1234ëŠ” PID (í”„ë¡œì„¸ìŠ¤ ID)

2. â“ ëˆ„ê°€ ê·¸ í¬íŠ¸ë¥¼ ì“°ëŠ”ì§€ í™•ì¸
tasklist /FI "PID eq 1234"
3. ğŸ›‘ í•„ìš” ì—†ëŠ” í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
taskkill /PID 1234 /F
â€» ì¤‘ìš”í•œ í”„ë¡œì„¸ìŠ¤ë¼ë©´ ì¢…ë£Œí•˜ë©´ ì•ˆ ë˜ë‹ˆê¹Œ ë­”ì§€ í™•ì¸í•˜ê³  ê²°ì •í•´.

ğŸ©¹ ì„ì‹œ ìš°íšŒ ë°©ë²• (ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©)
docker-compose.yamlì—ì„œ 8080 ëŒ€ì‹  ë‹¤ë¥¸ í¬íŠ¸(ex. 8081 ë“±)ë¡œ ë°”ê¿”ë„ ë¨:

ports:
  - "8081:8080"
ê·¸ í›„ì—” http://localhost:8081ë¡œ ì ‘ì†.

í•„ìš”í•˜ë©´ docker-compose.yaml íŒŒì¼ ì˜¬ë ¤ì¤˜. í¬íŠ¸ ì„¤ì • í™•ì¸í•´ì„œ ìˆ˜ì • ë°©í–¥ ì•Œë ¤ì¤„ê²Œ.

ë‚˜ì˜ ë§:
exec -it 00f8ee46b83f sh ë¥¼ ì´ìš©í•´ì„œ ë„ì»¤ ë‚´ë¶€ë¡œ ë“¤ì–´ê°€ì„œì‘ì—…ì„ í•˜ë ¤ëŠ”ë° 'exec'ì€(ëŠ”) ë‚´ë¶€ ë˜ëŠ” ì™¸ë¶€ ëª…ë ¹, ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í”„ë¡œê·¸ë¨, ë˜ëŠ”
ë°°ì¹˜ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.ë¼ê³  ë– 
ChatGPTì˜ ë§:
ì˜¤ë¥˜ ë©”ì‹œì§€ë¡œ ë´ì„œ, exec ëª…ë ¹ì–´ ìì²´ë¥¼ Windows ëª…ë ¹ í”„ë¡¬í”„íŠ¸(cmd)ì—ì„œ ì§ì ‘ ì³¤ê¸° ë•Œë¬¸ì— ë°œìƒí•œ ë¬¸ì œì•¼.

ğŸ§¨ ì˜¤ë¥˜ ì›ì¸
'exec'ì€(ëŠ”) ë‚´ë¶€ ë˜ëŠ” ì™¸ë¶€ ëª…ë ¹, ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í”„ë¡œê·¸ë¨, ë˜ëŠ” ë°°ì¹˜ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.
ì´ê±´ Windowsì—ì„œ execë¼ëŠ” ëª…ë ¹ì´ ì—†ê¸° ë•Œë¬¸ì— ë‚˜ì˜¤ëŠ” ì˜¤ë¥˜ê³ ,
ì‹¤ì œë¡œ ë„ì»¤ ì»¨í…Œì´ë„ˆ ì•ˆì— ë“¤ì–´ê°€ë ¤ë©´ docker exec ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•´ì•¼ í•´.

âœ… í•´ê²° ë°©ë²•
ì •í™•í•œ ëª…ë ¹ì–´ëŠ” ì•„ë˜ì™€ ê°™ì•„:

docker exec -it 00f8ee46b83f sh
í˜¹ì€ bash ì…¸ì´ ì„¤ì¹˜ëœ ì»¨í…Œì´ë„ˆë¼ë©´:

docker exec -it 00f8ee46b83f bash
dockerëŠ” ëª…ë ¹ì–´ì˜ ë§¨ ì•ì— ë°˜ë“œì‹œ ë¶™ì–´ì•¼ í•´.

ğŸ” ë§Œì•½ ì—¬ì „íˆ ì•ˆ ëœë‹¤ë©´?
1. Docker ë°ëª¬ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
docker ps
ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ docker execì´ ë™ì‘í•´.

2. PowerShellì´ë¼ë©´ sh í˜¸ì¶œ ì‹œ ë”°ì˜´í‘œë¡œ ê°ì‹¸ì•¼ í•  ìˆ˜ ìˆìŒ:
docker exec -it 00f8ee46b83f "sh"
í•„ìš”í•˜ë©´ ì •í™•í•œ ëª…ë ¹ì–´ ì‹¤í–‰ í™˜ê²½ (cmdì¸ì§€, PowerShellì¸ì§€, WSLì¸ì§€) ë§í•´ì¤˜. ê·¸ì— ë§ì¶° ë” ì •í™•í•˜ê²Œ ì•Œë ¤ì¤„ê²Œ.

ë‚˜ì˜ ë§:
Connections
â—‹ This is used to store some connection related info such as hostname, port number, and
access credential
â—‹ Postgres connection or Redshift connection info can be stored here
â— Variables
â—‹ Used to store API keys or some configuration info
â—‹ Use â€œaccessâ€ or â€œsecretâ€ in the name if you want its value to be encrypted
â—‹ We will practice this
í•´ì„í•´ì¤˜
ChatGPTì˜ ë§:
ë‹¤ìŒì€ í•´ë‹¹ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•˜ê²Œ í•œê¸€ë¡œ í•´ì„í•œ ê±°ì•¼:

ğŸ”— Connections (ì—°ê²° ì„¤ì •)
í˜¸ìŠ¤íŠ¸ëª…, í¬íŠ¸ ë²ˆí˜¸, ì ‘ê·¼ ì¸ì¦ ì •ë³´ ë“±ê³¼ ê°™ì€ ì—°ê²° ê´€ë ¨ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë¨

ì˜ˆë¥¼ ë“¤ì–´, Postgresë‚˜ Redshiftì™€ ê°™ì€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ë¥¼ ì´ê³³ì— ì €ì¥í•  ìˆ˜ ìˆìŒ

ğŸ” Variables (ë³€ìˆ˜)
API í‚¤ë‚˜ êµ¬ì„± ì„¤ì • ì •ë³´(configuration info) ë“±ì„ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë¨

ë³€ìˆ˜ ì´ë¦„ì— "access" ë˜ëŠ” **"secret"**ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´, ê·¸ ê°’ì€ ìë™ìœ¼ë¡œ ì•”í˜¸í™”ë¨

ì´ í•­ëª©ì€ ì‹¤ìŠµì„ í†µí•´ ë°°ì›Œë³¼ ì˜ˆì •ì„

í•„ìš”í•˜ë‹¤ë©´ ì˜ˆì œë„ ê°™ì´ ë“¤ì–´ì¤„ ìˆ˜ ìˆì–´.

ë‚˜ì˜ ë§:
cp -r learn-airflow/dags/* dags/
'cp'ì€(ëŠ”) ë‚´ë¶€ ë˜ëŠ” ì™¸ë¶€ ëª…ë ¹, ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í”„ë¡œê·¸ë¨, ë˜ëŠ”
ë°°ì¹˜ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤
ë¼ê³  ëœ¨ë„¤ 
í˜„ì¬ C:\Users\ryanp\OneDrive\ë°”íƒ• í™”ë©´\kdt\airflow-setupì— ìˆëŠ” learn-airflow í´ë” ì•ˆì— ìˆëŠ” dags ì „ì²´ë¥¼ airflow-setup/dagsë¡œ ë³µì‚¬í•˜ë ¤ê³ í•˜ëŠ”ë° ì™œì´ë˜
ChatGPTì˜ ë§:
ì—ëŸ¬ ë©”ì‹œì§€ë¡œ ë´¤ì„ ë•Œ, **Windows ëª…ë ¹ í”„ë¡¬í”„íŠ¸(cmd)**ì—ì„œ cp ëª…ë ¹ì–´ë¥¼ ì³¤ê¸° ë•Œë¬¸ì— ë°œìƒí•œ ë¬¸ì œì•¼.

ğŸ§¨ ì˜¤ë¥˜ ì›ì¸
'cp'ì€(ëŠ”) ë‚´ë¶€ ë˜ëŠ” ì™¸ë¶€ ëª…ë ¹, ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í”„ë¡œê·¸ë¨, ë˜ëŠ” ë°°ì¹˜ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤
cpëŠ” Linux/macOSì—ì„œ ì‚¬ìš©í•˜ëŠ” íŒŒì¼ ë³µì‚¬ ëª…ë ¹ì–´ì•¼.

Windowsì˜ ê¸°ë³¸ ëª…ë ¹ í”„ë¡¬í”„íŠ¸ì—ì„œëŠ” cp ëŒ€ì‹  xcopy ë˜ëŠ” robocopyë¥¼ ì¨ì•¼ í•´.

âœ… í•´ê²° ë°©ë²• (Windows CMD ê¸°ì¤€)
ë°©ë²• 1: xcopy ì‚¬ìš©
xcopy /E /I "learn-airflow\dags\*" "dags\"
ì˜µì…˜ ì„¤ëª…:

/E: í•˜ìœ„ í´ë”ê¹Œì§€ í¬í•¨

/I: ëŒ€ìƒì´ ë””ë ‰í„°ë¦¬ë¼ê³  ê°„ì£¼

\*: ì „ì²´ íŒŒì¼ í¬í•¨

ë°©ë²• 2: robocopy ì‚¬ìš© (ë” ì•ˆì •ì ì„)
robocopy "learn-airflow\dags" "dags" /E
ğŸ› ï¸ ëŒ€ì•ˆ: Git Bash ë˜ëŠ” WSL ì‚¬ìš©
ë§Œì•½ cp, ls, mv ê°™ì€ Linux ìŠ¤íƒ€ì¼ ëª…ë ¹ì–´ë¥¼ ìì£¼ ì“°ê³  ì‹¶ë‹¤ë©´:

Git Bash ì„¤ì¹˜ (ê¶Œì¥): https://git-scm.com

ë˜ëŠ” WSL (Windows Subsystem for Linux) ì„¤ì¹˜

ì´í›„ Git Bashë‚˜ WSL í„°ë¯¸ë„ì—ì„œëŠ” cp -rì„ ê·¸ëŒ€ë¡œ ì“¸ ìˆ˜ ìˆì–´:

cp -r learn-airflow/dags/* dags/
í•„ìš”í•˜ë©´ ì–´ë–¤ í„°ë¯¸ë„ì—ì„œ ì‘ì—…í•˜ê³  ìˆëŠ”ì§€ë„ ë§í•´ì¤˜. ê·¸ì— ë§ëŠ” ëª…ë ¹ì–´ë¡œ ì •ë¦¬í•´ì¤„ê²Œ.

ë‚˜ì˜ ë§:
ë„ì»¤ã…” airflowë¥¼ ì„¤ì¹˜ë¥¼ í–ˆëŠ”ë° airflowê°€ ì‚¬ìš©í•˜ëŠ” dagë“¤ì„ ë‹´ëŠ” í´ë”ë¥¼ ì§€ì •í•˜ëŠ” í‚¤ëŠ” ì–´ë””ì„œ í™•ã…‡ë‹ˆí•´?
ChatGPTì˜ ë§:
Airflowì—ì„œ DAG íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” ë””ë ‰í† ë¦¬(í´ë”)ëŠ” Airflow ì„¤ì • íŒŒì¼ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì§€ì •ë©ë‹ˆë‹¤.

âœ… DAG í´ë” ìœ„ì¹˜ í™•ì¸ ë°©ë²•
1. airflow.cfg íŒŒì¼ì—ì„œ í™•ì¸
Airflowê°€ ì„¤ì¹˜ëœ ì»¨í…Œì´ë„ˆë‚˜ í™˜ê²½ ë‚´ì—ì„œ ë‹¤ìŒ ê²½ë¡œì˜ ì„¤ì • íŒŒì¼ì„ ë´:

# airflow.cfg ë‚´ë¶€
dags_folder = /opt/airflow/dags
ì¦‰, ê¸°ë³¸ì ìœ¼ë¡œëŠ” /opt/airflow/dags ê²½ë¡œë¥¼ ì‚¬ìš©í•¨.
â€» ì´ ê²½ë¡œëŠ” Docker ê³µì‹ ì´ë¯¸ì§€ì—ì„œë„ ë™ì¼í•¨.

2. í™˜ê²½ ë³€ìˆ˜ì—ì„œ í™•ì¸ (ë„ì»¤ í™˜ê²½ì´ë¼ë©´ ë³´í†µ ì´ê²Œ ìš°ì„ ë¨)
AirflowëŠ” AIRFLOW__CORE__DAGS_FOLDER í™˜ê²½ ë³€ìˆ˜ë¡œë„ DAG ê²½ë¡œë¥¼ ì§€ì •í•  ìˆ˜ ìˆì–´.

echo $AIRFLOW__CORE__DAGS_FOLDER
Windows PowerShellì—ì„œëŠ”:

$Env:AIRFLOW__CORE__DAGS_FOLDER
ë˜ëŠ” docker-compose.yamlì—ì„œ ë‹¤ìŒì²˜ëŸ¼ ë˜ì–´ìˆì„ ìˆ˜ë„ ìˆìŒ:

environment:
  - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
ğŸ” ë„ì»¤ í™˜ê²½ì—ì„œ DAG ê²½ë¡œ í™•ì¸
1. ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆ ì•ˆìœ¼ë¡œ ì ‘ì†:
docker exec -it <ì»¨í…Œì´ë„ˆID or ì´ë¦„> bash
2. ì„¤ì • í™•ì¸
cat $AIRFLOW_HOME/airflow.cfg | grep dags_folder
ë˜ëŠ”:

echo $AIRFLOW__CORE__DAGS_FOLDER
ğŸ“ ìš”ì•½
í™•ì¸ ë°©ë²•	ë‚´ìš©
airflow.cfg	dags_folder = ... í‚¤ë¥¼ í™•ì¸
í™˜ê²½ë³€ìˆ˜	AIRFLOW__CORE__DAGS_FOLDER ê°’ì„ í™•ì¸
ê¸°ë³¸ê°’	/opt/airflow/dags (ê³µì‹ Docker ì´ë¯¸ì§€ ê¸°ì¤€)
docker-compose.yamlì´ë‚˜ airflow.cfg íŒŒì¼ ë³´ì—¬ì£¼ë©´ ì •í™•íˆ ì–´ë””ì— ì„¤ì •ë¼ ìˆëŠ”ì§€ë„ ë°”ë¡œ ì•Œë ¤ì¤„ ìˆ˜ ìˆì–´.

ë‚˜ì˜ ë§:
ì ‘ê·¼ ê±°ë¶€ë¼ê³  ëœ¨ëŠë„¤?
ChatGPTì˜ ë§:
â€œì ‘ê·¼ ê±°ë¶€â€ê°€ ë–´ë‹¤ëŠ” ê±´ íŒŒì¼ì„ ë³´ê±°ë‚˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•  ê¶Œí•œì´ ì—†ë‹¤ëŠ” ê²ƒì¸ë°, ìƒí™©ì— ë”°ë¼ ì›ì¸ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ë¨¼ì € ì •í™•í•œ ë§¥ë½ì´ í•„ìš”í•´.

ğŸ” ìƒí™©ë³„ ì›ì¸ ì •ë¦¬
â¶ docker execë¡œ ë“¤ì–´ê°€ì„œ cat airflow.cfg í•  ë•Œ ì ‘ê·¼ ê±°ë¶€ë¼ë©´?
íŒŒì¼ ê²½ë¡œê°€ í‹€ë ¸ê±°ë‚˜

í˜„ì¬ ë¡œê·¸ì¸ëœ ì‚¬ìš©ìì— ì½ê¸° ê¶Œí•œì´ ì—†ìŒ

ğŸ‘‰ í•´ê²°:

# airflow.cfgê°€ ì–´ë”” ìˆëŠ”ì§€ë¶€í„° í™•ì¸
find / -name airflow.cfg 2>/dev/null
ë³´í†µ ë„ì»¤ ê³µì‹ airflow ì´ë¯¸ì§€ì—ì„œëŠ”:

cat /opt/airflow/airflow.cfg
â· docker exec ìì²´ê°€ ì ‘ê·¼ ê±°ë¶€ì¼ ê²½ìš°?
docker exec -it ì»¨í…Œì´ë„ˆID bash
Docker ë°ëª¬ì´ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•Šê±°ë‚˜

ê´€ë¦¬ì ê¶Œí•œ(cmd/Powershellì„ "ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰")ì´ í•„ìš”

ğŸ‘‰ í•´ê²°:

CMDë‚˜ PowerShellì„ ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰

Docker ë°ëª¬ì´ ì¼œì ¸ ìˆëŠ”ì§€ í™•ì¸

âœ… í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸
í•­ëª©	í™•ì¸ ë°©ë²•
ê´€ë¦¬ì ê¶Œí•œ	CMDë¥¼ "ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰"
ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ì¤‘ì¸ì§€	docker ps
ì»¨í…Œì´ë„ˆì— ì§„ì… ê°€ëŠ¥í•œì§€	docker exec -it <ID> bash ë˜ëŠ” sh
airflow.cfg ê²½ë¡œ	/opt/airflow/airflow.cfg
ì •í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€(ì˜ˆ: "Permission denied", "Access is denied", "Operation not permitted")ì™€ ì–´ë–¤ ëª…ë ¹ì–´ì—ì„œ ë–´ëŠ”ì§€ë„ ì•Œë ¤ì£¼ë©´, í›¨ì”¬ ì •í™•í•˜ê²Œ ë„ì™€ì¤„ ìˆ˜ ìˆì–´.
í•„ìš”í•˜ë©´ ê·¸ ë©”ì‹œì§€ë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ì„œ ë³´ë‚´ì¤˜.

ë‚˜ì˜ ë§:
cat /opt/airflow/airflow.cfg
ë¡œ í•´ë‹¹ íŒŒì¼ì„ ì—´ì—ˆëŠ”ë° ë‚´ë¶€ì—ì„œ 5ë¶„ë§ˆë‹¤ ë””í´íŠ¸ë¡œ ìƒˆë¡œìš´ dagê°€ ìˆëŠ”ì§€ ìŠ¤ìº”í•œë‹¤í•˜ëŠ”ë° ê·¸ëŸ¼ ê·¸ ìŠ¤ìº” ì£¼ê¸°ë¥¼ê²°ì¡í•´ì£¼ëŠ” í‚¤ë¥¼ í™•ì¸í•˜ê³  ì‹œã…ã…ì€ë° airflow.cfg ì•ˆì— ìˆì„ê¹Œ/
ChatGPTì˜ ë§:
ì •í™•íˆ ì§šì—ˆì–´.
AirflowëŠ” ì£¼ê¸°ì ìœ¼ë¡œ DAG í´ë”ë¥¼ ìŠ¤ìº”í•´ì„œ ìƒˆë¡œìš´ DAG íŒŒì¼ì„ ë¡œë“œí•˜ëŠ”ë°, ê·¸ **ì£¼ê¸°(ì´ˆ ë‹¨ìœ„)**ë¥¼ ì„¤ì •í•˜ëŠ” í‚¤ëŠ” ë‹¤ìŒê³¼ ê°™ì•„:

ğŸ” DAG ìŠ¤ìº” ì£¼ê¸° ì„¤ì • í‚¤
# airflow.cfg
scheduler:
dag_dir_list_interval = 300
ê¸°ë³¸ê°’ì€ 300ì´ˆ = 5ë¶„

ì´ ê°’ì´ DAG ë””ë ‰í† ë¦¬ë¥¼ ì–¼ë§ˆë‚˜ ìì£¼ ìŠ¤ìº”í• ì§€ ê²°ì •í•´

ğŸ“ ì˜ˆì‹œ
[scheduler]
# The scheduler will scan the DAGs folder every 60 seconds
dag_dir_list_interval = 60
ì´ë ‡ê²Œ ì„¤ì •í•˜ë©´ 1ë¶„ë§ˆë‹¤ DAG ë””ë ‰í† ë¦¬ë¥¼ ì¬ìŠ¤ìº”í•¨.

âœ… ì„¤ì • ì ìš© ë°©ë²•
airflow.cfg íŒŒì¼ ìˆ˜ì • í›„

Schedulerë¥¼ ì¬ì‹œì‘í•´ì•¼ ì ìš©ë¨:

docker-compose restart scheduler
ë˜ëŠ”:

docker restart <scheduler-container-id>
í•„ìš”í•˜ë©´ airflow.cfg íŒŒì¼ ì¼ë¶€ë¥¼ ê³µìœ í•´ì£¼ë©´, ê°™ì´ í™•ì¸í•´ì¤„ê²Œ.
ë˜í•œ airflow 2.0 ì´ìƒì—ì„œëŠ” airflow config get-value scheduler dag_dir_list_intervalë¡œë„ í˜„ì¬ ì„¤ì •ê°’ì„ í™•ì¸í•  ìˆ˜ ìˆì–´.

ë‚˜ì˜ ë§:
ê·¸ëŸ¼ airflow.cfg ë‚´ë¶€ì— ì–´ëŠ í´ë”ì˜ dagë¥¼  ì‚¬ìš©í• ì§€ ì§€ì •í•˜ëŠ í‚¤ëŠ” ë­ì•¼
ChatGPTì˜ ë§:
Airflowê°€ ì–´ë–¤ ë””ë ‰í† ë¦¬(DAG í´ë”)ì—ì„œ DAG íŒŒì¼ì„ ë¡œë“œí• ì§€ë¥¼ ì§€ì •í•˜ëŠ” ì„¤ì • í‚¤ëŠ” ë‹¤ìŒê³¼ ê°™ì•„:

âœ… DAG í´ë” ì§€ì • í‚¤
[core]
dags_folder = /opt/airflow/dags
ì´ ì„¤ì •ì€ Airflowê°€ DAG íŒŒì¼(.py)ì„ ì–´ë””ì„œ ì½ì„ì§€ ì§€ì •í•¨

ë³´í†µ airflow.cfgì˜ [core] ì„¹ì…˜ì— ìˆìŒ

ğŸ” í™•ì¸ ë°©ë²• (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ)
grep dags_folder /opt/airflow/airflow.cfg
ì˜ˆì‹œ ê²°ê³¼:

dags_folder = /opt/airflow/dags
ğŸ›  ë³€ê²½ ë°©ë²•
airflow.cfgì˜ dags_folder ê°’ì„ ì›í•˜ëŠ” ê²½ë¡œë¡œ ìˆ˜ì •

DAG íŒŒì¼ë“¤ì„ í•´ë‹¹ ê²½ë¡œë¡œ ì´ë™

Airflow Scheduler ì¬ì‹œì‘:

docker-compose restart scheduler
ğŸ” ëŒ€ì²´: í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •ë„ ê°€ëŠ¥
ë„ì»¤ í™˜ê²½ì´ë¼ë©´ í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ í•  ìˆ˜ ìˆìŒ:

# docker-compose.yaml ì˜ˆì‹œ
environment:
  - AIRFLOW__CORE__DAGS_FOLDER=/your/custom/dags/path
í•„ìš”í•˜ë©´ ë„¤ê°€ ì“°ëŠ” docker-compose.yamlì„ ë³´ì—¬ì¤˜. ê±°ê¸°ì„œ DAG ê²½ë¡œë„ ì„¤ì •ë˜ì–´ ìˆì„ ìˆ˜ ìˆì–´.

ë‚˜ì˜ ë§:
airflowê°€ ì‚¬ìš©í•˜ëŠ” dagë“¤ì€ ì–´ëŠ í´ë”ì— ì €ì¥ë˜ëŠ”ì§€ ì§€ì •í•˜ëŠ” í‚¤ë¥¼ ì•Œê³  ì‹¶ì–´
ChatGPTì˜ ë§:
ëª…í™•í•˜ê²Œ ëŒ€ë‹µí• ê²Œ.

âœ… Airflowê°€ ì‚¬ìš©í•˜ëŠ” DAG í´ë”ë¥¼ ì§€ì •í•˜ëŠ” ì„¤ì • í‚¤ëŠ”:
[core]
dags_folder = /opt/airflow/dags
ì´ê²Œ ë°”ë¡œ Airflowê°€ DAG íŒŒì¼(.py)ì„ ìŠ¤ìº”í•˜ëŠ” í´ë”ë¥¼ ì§€ì •í•˜ëŠ” í•µì‹¬ í‚¤ì•¼.

ğŸ” í™•ì¸ ë°©ë²•
ë„ì»¤ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

grep dags_folder /opt/airflow/airflow.cfg
ì¶œë ¥ ì˜ˆ:

dags_folder = /opt/airflow/dags
ì´ ê²½ë¡œì— .py íŒŒì¼ì„ ë„£ìœ¼ë©´ DAGë¡œ ì¸ì‹ë¼.

ğŸ” ë³€ê²½í•˜ê³  ì‹¶ë‹¤ë©´?
airflow.cfg íŒŒì¼ì„ ì—´ê³  dags_folder ê²½ë¡œë¥¼ ìˆ˜ì •í•œ ë’¤:

docker-compose restart scheduler
ë˜ëŠ” ë„ì»¤ í™˜ê²½ì´ë©´ docker-compose.yamlì—ì„œ ì•„ë˜ì²˜ëŸ¼ í™˜ê²½ ë³€ìˆ˜ë¡œë„ ì„¤ì • ê°€ëŠ¥:

environment:
  - AIRFLOW__CORE__DAGS_FOLDER=/custom/dags
ğŸ“Œ ìš”ì•½
í•­ëª©	ê°’
ì„¤ì • í‚¤	dags_folder
ìœ„ì¹˜	[core] ì„¹ì…˜
ìš©ë„	DAG íŒŒì¼ë“¤ì´ ì €ì¥ë  ë””ë ‰í† ë¦¬ ì§€ì •
ê¸°ë³¸ê°’	/opt/airflow/dags (ê³µì‹ ë„ì»¤ ì´ë¯¸ì§€ ê¸°ì¤€)
í•„ìš”í•˜ë©´ ë„¤ êµ¬ì„±íŒŒì¼ì— ë§ì¶° ê²½ë¡œ ìˆ˜ì •í•˜ëŠ” ê²ƒë„ ë„ì™€ì¤„ê²Œ.

ë‚˜ì˜ ë§:
dags_folder = /opt/airflow/dags ê·¸ëŸ¼ ì´ê±´ ì–´ë””ì„œ í™•ì¸í•´
ChatGPTì˜ ë§:
ì¢‹ì•„, ì§ˆë¬¸ì— ëª…í™•íˆ ë‹µí•´ì¤„ê²Œ.

âœ… dags_folder = /opt/airflow/dags ê°’ì€ airflow.cfg íŒŒì¼ì—ì„œ í™•ì¸ ê°€ëŠ¥í•´.
ğŸ“ ê²½ë¡œ (ë„ì»¤ ê¸°ì¤€)
ê³µì‹ Docker ì´ë¯¸ì§€ì—ì„œëŠ” airflow.cfgëŠ” ë‹¤ìŒ ê²½ë¡œì— ìˆì–´:

/opt/airflow/airflow.cfg
ğŸ” í™•ì¸ ë°©ë²•
1. ë„ì»¤ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ë¡œ ì ‘ì†
docker exec -it <ì»¨í…Œì´ë„ˆ ì´ë¦„ ë˜ëŠ” ID> bash
ì˜ˆ:

docker exec -it airflow-scheduler bash
2. ì„¤ì • íŒŒì¼ì—ì„œ ê²€ìƒ‰
grep dags_folder /opt/airflow/airflow.cfg
ì¶œë ¥ ì˜ˆì‹œ:

dags_folder = /opt/airflow/dags
â›³ ë³´ë„ˆìŠ¤: ëª…ë ¹ì–´ í•œ ì¤„ë¡œ DAG í´ë” ìœ„ì¹˜ í™•ì¸
Airflow 2.0 ì´ìƒì´ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œë„ ë°”ë¡œ í™•ì¸ ê°€ëŠ¥:

airflow config get-value core dags_folder
ì¶œë ¥ ì˜ˆ:

/opt/airflow/dags
í•„ìš”í•˜ë©´ docker-compose.yaml íŒŒì¼ë„ ê°™ì´ í™•ì¸í•´ì„œ í˜¸ìŠ¤íŠ¸ì™€ì˜ ê³µìœ  ê²½ë¡œ(volumes)ê¹Œì§€ í™•ì¸í•´ì¤„ ìˆ˜ ìˆì–´. ë³´ì—¬ì¤„ë˜?

ë‚˜ì˜ ë§:
airflowë¥¼ api í˜•íƒœë¡œ ì™¸ë¶€ì—ì„œ ì¡°ì‘í•˜ê³  ì‹¶ë‹¤ë©´ ì–´ëŠ ì„¹ì…˜ì„ ë³€ê²½í•´ì•¼í•˜ëŠ”ê°€ authantication ë¶€ë¶„ ê°™ì€ê±°
ChatGPTì˜ ë§:
ì¢‹ì€ ì§ˆë¬¸ì´ì•¼.
Airflowë¥¼ ì™¸ë¶€ì—ì„œ REST APIë¡œ ì¡°ì‘í•˜ë ¤ë©´, **Webserver ì„¤ì • ([webserver])ê³¼ API authentication ì„¤ì • ([api])**ì„ êµ¬ì„±í•´ì•¼ í•´.

âœ… Airflow ì™¸ë¶€ API ì‚¬ìš©ì„ ìœ„í•œ ì„¤ì • ìš”ì•½
1. airflow.cfgì—ì„œ ë³€ê²½í•  ì£¼ìš” ì„¹ì…˜
ğŸ“Œ [webserver] ì„¹ì…˜
[webserver]
authenticate = True
auth_backend = airflow.api.auth.backend.basic_auth
authenticate = True: ì¸ì¦ ì‚¬ìš©

auth_backend: ì‚¬ìš©í•  ì¸ì¦ ë°©ì‹ (ê¸°ë³¸ì€ basic_auth)

auth_backend ì˜ˆì‹œ:

airflow.api.auth.backend.default â†’ ì¸ì¦ ì—†ì´ ì‚¬ìš© (ë³´ì•ˆ ì·¨ì•½)

airflow.api.auth.backend.basic_auth â†’ ê¸°ë³¸ ì‚¬ìš©ì ì¸ì¦ (Admin ê³„ì • í•„ìš”)

ğŸ“Œ [api] ì„¹ì…˜
[api]
auth_backends = airflow.api.auth.backend.basic_auth
API ìš”ì²­ì— ì‚¬ìš©í•  ì¸ì¦ ë°±ì—”ë“œ ì§€ì •

ê¸°ë³¸ê°’ì€ airflow.api.auth.backend.default (ì¸ì¦ ì—†ì´ ì ‘ê·¼ ê°€ëŠ¥ â†’ ì ˆëŒ€ ë¹„ì¶”ì²œ)

âœ… ê¶Œì¥ ì¸ì¦ ì„¤ì • (ë³´ì•ˆ ì ìš© ì˜ˆ)
[webserver]
authenticate = True
auth_backend = airflow.api.auth.backend.basic_auth

[api]
auth_backends = airflow.api.auth.backend.basic_auth
âœ… Admin ì‚¬ìš©ì ìƒì„± (API ì ‘ê·¼ìš©)
airflow users create \
  --username admin \
  --firstname FIRST \
  --lastname LAST \
  --role Admin \
  --email admin@example.com \
  --password yourpassword
ì´ ì‚¬ìš©ì ê³„ì •ìœ¼ë¡œ Basic Auth ì¸ì¦ì„ í†µí•´ API ì ‘ê·¼ ê°€ëŠ¥í•¨.

âœ… API í˜¸ì¶œ ì˜ˆ (cURL)
curl -X GET "http://localhost:8080/api/v1/dags" \
  --user "admin:yourpassword"
ğŸ“ ì°¸ê³  ê²½ë¡œ
Airflow ê³µì‹ API ë¬¸ì„œ:
https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html

ğŸ“ ìš”ì•½
ì„¹ì…˜	í‚¤	ì„¤ëª…
[webserver]	authenticate	ì¸ì¦ ì‚¬ìš© ì—¬ë¶€
[webserver]	auth_backend	ì¸ì¦ ë°©ì‹ ì§€ì •
[api]	auth_backends	API ì¸ì¦ ë°±ì—”ë“œ ì§€ì •
í•„ìš”í•˜ë©´ docker-compose.yamlì—ì„œ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ëŠ” ë°©ë²•ë„ ì•Œë ¤ì¤„ ìˆ˜ ìˆì–´. ë³´ì—¬ì¤„ê¹Œ?

ë‚˜ì˜ ë§:
[api]
# Enables the deprecated experimental API. Please note that these APIs do not have access control.
# The authenticated user has full access.
#
# .. warning::
#
#   This Experimental REST API <https://airflow.readthedocs.io/en/latest/rest-api-ref.html>__ is
#   deprecated since version 2.0. Please consider using
#   the Stable REST API <https://airflow.readthedocs.io/en/latest/stable-rest-api-ref.html>__.
#   For more information on migration, see
#   RELEASE_NOTES.rst <https://github.com/apache/airflow/blob/main/RELEASE_NOTES.rst>_
#
# Variable: AIRFLOW__API__ENABLE_EXPERIMENTAL_API
#
enable_experimental_api = False

# Comma separated list of auth backends to authenticate users of the API. See
# https://airflow.apache.org/docs/apache-airflow/stable/security/api.html for possible values.
# ("airflow.api.auth.backend.default" allows all requests for historic reasons)
#
# Variable: AIRFLOW__API__AUTH_BACKENDS
#
auth_backends = airflow.api.auth.backend.session
ì´ë¶€ë¶„ì¸ê°€?
ChatGPTì˜ ë§:
ë„¤, ë§ì•„. ì§€ê¸ˆ ë³´ì—¬ì¤€ [api] ì„¹ì…˜ì´ ë°”ë¡œ ì™¸ë¶€ì—ì„œ APIë¡œ Airflowë¥¼ ì¡°ì‘í•  ë•Œ ì¤‘ìš”í•œ ì„¤ì • ë¶€ë¶„ì´ì•¼. ì•„ë˜ì— ìƒì„¸í•˜ê²Œ ì •ë¦¬í•´ì¤„ê²Œ.

âœ… ë„¤ê°€ ë¶™ì—¬ì¤€ ì„¤ì • ì„¤ëª…
ğŸ”¸ enable_experimental_api = False
**ì‹¤í—˜ìš© API(Deprecated)**ë¥¼ ì‚¬ìš©í• ì§€ ì—¬ë¶€

Airflow 2.0ë¶€í„°ëŠ” /api/v1 Stable REST APIê°€ ê¸°ë³¸ì´ë©°,
ì´ ì‹¤í—˜ìš© APIëŠ” ë” ì´ìƒ ê¶Œì¥ë˜ì§€ ì•ŠìŒ â†’ False ìœ ì§€ ê¶Œì¥

ğŸ”¸ auth_backends = airflow.api.auth.backend.session
ì´ê±´ API ìš”ì²­ ì‹œ ì‚¬ìš©í•  ì¸ì¦ ë°©ì‹ì„ ì§€ì •í•˜ëŠ” ì„¤ì •ì´ì•¼.

airflow.api.auth.backend.session
ë¸Œë¼ìš°ì € ê¸°ë°˜ ì„¸ì…˜ ì¸ì¦ ë°©ì‹ (CSRF í† í°ì´ í•„ìš”í•˜ê³ , REST API í˜¸ì¶œì— ë¶ˆí¸)

ì¦‰, ë¸Œë¼ìš°ì €ë¥¼ í†µí•´ ë¡œê·¸ì¸í•œ ì‚¬ìš©ìì˜ ì„¸ì…˜ìœ¼ë¡œë§Œ API ì ‘ê·¼ ê°€ëŠ¥

âœ… ë°”ê¾¸ê³  ì‹¶ë‹¤ë©´?
APIë¥¼ curlì´ë‚˜ ì™¸ë¶€ ì•±ì—ì„œ ì§ì ‘ í˜¸ì¶œí•˜ë ¤ë©´ ë³´í†µ ë‹¤ìŒìœ¼ë¡œ ë³€ê²½í•´:

auth_backends = airflow.api.auth.backend.basic_auth
ì´ë ‡ê²Œ í•˜ë©´ HTTP Basic ì¸ì¦ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ìëª…/ë¹„ë°€ë²ˆí˜¸ë¥¼ API ìš”ì²­ì— í¬í•¨í•´ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆì–´.

ğŸ“Œ ì˜ˆì‹œ ì„¤ì • (airflow.cfg)
[api]
enable_experimental_api = False
auth_backends = airflow.api.auth.backend.basic_auth

[webserver]
authenticate = True
auth_backend = airflow.api.auth.backend.basic_auth
ğŸ“Œ API í…ŒìŠ¤íŠ¸ (curl)
curl -X GET "http://localhost:8080/api/v1/dags" \
  --user "admin:yourpassword"
ğŸ’¡ ì •ë¦¬
í‚¤	ì„¤ëª…
auth_backends	API ì¸ì¦ ë°©ì‹ ì§€ì •
airflow.api.auth.backend.basic_auth	ì‚¬ìš©ìëª…/ë¹„ë°€ë²ˆí˜¸ë¡œ ì¸ì¦ (REST API í˜¸ì¶œì— ì í•©)
airflow.api.auth.backend.session	ë¸Œë¼ìš°ì € ì„¸ì…˜ ê¸°ë°˜ ì¸ì¦ (ë¹„ì¶”ì²œ)
ì¦‰, APIë¥¼ ì™¸ë¶€ì—ì„œ ì¡°ì‘í•˜ë ¤ë©´ auth_backendsë¥¼ basic_authë¡œ ë°”ê¾¸ê³ ,
**Airflow ì‚¬ìš©ì(admin ë“±)**ë„ ìƒì„±ë˜ì–´ ìˆì–´ì•¼ í•´.

ì›í•˜ë©´ docker-composeì—ì„œ ì´ê±¸ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ëŠ” ë°©ë²•ë„ ì•Œë ¤ì¤„ê²Œ.

ë‚˜ì˜ ë§:
airflowë¥¼ apií˜•íƒœë¡œ ì™¸ë¶€ì— ë…¸ì¶œ ì‹œí‚¤ê³  ì™¸ë¶€ì—ì„œ apië¥¼ ë¶ˆëŸ¬ì„œ ì¡°ì‘ì„ í•˜ë ¤ë©´ ì–´ë–¤ê°’ì„ìˆ˜ì •í•´ì•¼í•´
ChatGPTì˜ ë§:
ì¢‹ì•„, ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë°©í–¥ìœ¼ë¡œ ì •ë¦¬í•´ì¤„ê²Œ.
Airflowë¥¼ ì™¸ë¶€ APIë¡œ ë…¸ì¶œí•´ì„œ ì¡°ì‘í•˜ë ¤ë©´ ë‹¤ìŒ 3ê°€ì§€ë¥¼ ë°˜ë“œì‹œ ì„¤ì •í•´ì•¼ í•´:

âœ… 1. API ì¸ì¦ ë°©ì‹ ì„¤ì •
AirflowëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì™¸ë¶€ì—ì„œ REST APIë¡œ í˜¸ì¶œí•˜ë©´ ì¸ì¦ ì˜¤ë¥˜ê°€ ë‚©ë‹ˆë‹¤.
ë”°ë¼ì„œ ì¸ì¦ ë°©ì‹ì„ basic_authë¡œ ë³€ê²½í•´ì•¼ ì™¸ë¶€ ì•±ì´ë‚˜ curlì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•´ìš”.

ğŸ”§ airflow.cfg ì„¤ì •
[webserver]
authenticate = True
auth_backend = airflow.api.auth.backend.basic_auth

[api]
auth_backends = airflow.api.auth.backend.basic_auth
ìœ„ ì„¤ì •ì€ REST API í˜¸ì¶œ ì‹œ ID/PW ê¸°ë°˜ ì¸ì¦ì„ í—ˆìš©í•˜ê²Œ í•´ì¤ë‹ˆë‹¤.

âœ… 2. ë„ì»¤ í™˜ê²½ì´ë©´ í™˜ê²½ë³€ìˆ˜ë¡œ ì ìš©
airflow.cfg ì§ì ‘ ì•ˆ ê±´ë“œë¦¬ê³ , docker-compose.yaml íŒŒì¼ì— í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤:

services:
  airflow-webserver:
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__AUTHENTICATE=True
      - AIRFLOW__WEBSERVER__AUTH_BACKEND=airflow.api.auth.backend.basic_auth
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
ë³€ê²½ í›„ ë°˜ë“œì‹œ:

docker-compose down
docker-compose up -d
ìœ¼ë¡œ ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ í•„ìš”.

âœ… 3. Admin ì‚¬ìš©ì ìƒì„±
API í˜¸ì¶œì— ì‚¬ìš©í•  ì‚¬ìš©ìë¥¼ ë§Œë“¤ì–´ì•¼ í•´:

docker exec -it <webserver-container> airflow users create \
  --username admin \
  --firstname FIRST \
  --lastname LAST \
  --role Admin \
  --email admin@example.com \
  --password yourpassword
âœ… 4. í¬íŠ¸ ì™¸ë¶€ ë…¸ì¶œ í™•ì¸ (ê¸°ë³¸ 8080)
docker-compose.yamlì—ì„œ ì›¹ ì„œë²„ í¬íŠ¸ë¥¼ ì™¸ë¶€ì— ë…¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.

ports:
  - "8080:8080"
ì™¸ë¶€ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•´ì•¼ í•˜ë¯€ë¡œ:

ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸ë©´ localhost:8080

í´ë¼ìš°ë“œë‚˜ VMì´ë©´ í•´ë‹¹ ì„œë²„ì˜ í¼ë¸”ë¦­ IP:8080

âœ… 5. API í˜¸ì¶œ ì˜ˆ (Basic Auth)
curl -X GET "http://<airflow-host>:8080/api/v1/dags" \
  --user "admin:yourpassword"
ğŸ” ì£¼ì˜: ìš´ì˜ í™˜ê²½ì—ì„œëŠ” HTTPS + í† í° ê¸°ë°˜ ì¸ì¦ì´ í•„ìš”í•¨
Basic AuthëŠ” ID/PWê°€ ë…¸ì¶œë  ìˆ˜ ìˆìŒ

ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ì¸ì¦ í”„ë¡ì‹œ, HTTPS ì ìš©, role-based controlê¹Œì§€ ê³ ë ¤í•´ì•¼ ì•ˆì „í•¨

ğŸ“ ìµœì¢… ìš”ì•½
í•­ëª©	ì„¤ì •
ì¸ì¦ ë°©ì‹	auth_backends = airflow.api.auth.backend.basic_auth
docker-compose í™˜ê²½ë³€ìˆ˜	AIRFLOW__API__AUTH_BACKENDS, AIRFLOW__WEBSERVER__AUTH_BACKEND ë“±
ì‚¬ìš©ì ìƒì„±	airflow users create --role Admin ...
í¬íŠ¸ ë…¸ì¶œ	8080:8080
API í…ŒìŠ¤íŠ¸	curl + basic auth
docker-compose.yaml íŒŒì¼ ë³´ì—¬ì£¼ë©´ ìˆ˜ì •í•´ì„œ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ ì˜ˆì‹œë„ ë§Œë“¤ì–´ì¤„ ìˆ˜ ìˆì–´. í•„ìš”í•˜ë©´ ë¶™ì—¬ì¤˜.

ë‚˜ì˜ ë§:
admin ë°‘ì— Variableì—ì„œ ë³€ìˆ˜ì˜ ì´ë¦„ì´ ì–´ë–¤ ê°’ì´ ë“¤ì–´ê°€ìˆëƒì— ë”°ë¼ì„œ ê·¸ ë³€ìˆ˜ì˜ ê°’ì´ encodingì´ ë˜ê³  encodeingì´ ë˜ì§€ ì•ŠëŠ”ì§€, ì–´ë–¤ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ì•¼ ã„±ë°§ì´ encrypted ëœ í˜•íƒœë¡œ ë˜ëŠ”ê°€
ChatGPTì˜ ë§:
ì¢‹ì€ ì§ˆë¬¸ì´ì•¼.
Airflowì˜ Admin > Variables ì„¹ì…˜ì—ì„œ ì €ì¥í•˜ëŠ” ë³€ìˆ˜ ê°’ì€ íŠ¹ì • ì¡°ê±´ì¼ ë•Œ ìë™ìœ¼ë¡œ ì•”í˜¸í™”(Encryption) ë¼.

âœ… ì–´ë–¤ ê²½ìš°ì— ê°’ì´ ìë™ìœ¼ë¡œ ì•”í˜¸í™”(Encrypted) ë˜ëŠ”ê°€?
AirflowëŠ” ë³€ìˆ˜ ì´ë¦„ì— "secret" ë˜ëŠ” "access"ë¼ëŠ” ë‹¨ì–´ê°€ ë“¤ì–´ ìˆìœ¼ë©´, í•´ë‹¹ ê°’ì€ ì•”í˜¸í™”í•´ì„œ ì €ì¥í•´.

ğŸ”’ ì¡°ê±´:
ë³€ìˆ˜ ì´ë¦„ì— ë‹¤ìŒ ë¬¸ìì—´ì´ í¬í•¨ë˜ë©´:

"secret"

"access"

ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆ í•¨ (case-insensitive)

ğŸ” ì˜ˆì‹œ
ë³€ìˆ˜ ì´ë¦„	ì•”í˜¸í™” ì—¬ë¶€
my_api_key	âŒ No
access_token	âœ… Yes
SECRET_KEY	âœ… Yes
db_password	âŒ No
user_access_code	âœ… Yes
Airflow UIì—ì„œ ì•”í˜¸í™”ëœ ê°’ì€ ********ë¡œ í‘œì‹œë¨.

âœ… ì‹¤ì œ ì•”í˜¸í™” ì €ì¥ ë°©ì‹
ì•”í˜¸í™”ëœ ê°’ì€ Airflowì˜ FernÃ©t keyë¥¼ ì´ìš©í•´ ì•”í˜¸í™”ë¨.

ì´ í‚¤ëŠ” Airflow ì„¤ì¹˜ ì‹œ ìë™ ìƒì„±ë˜ë©°, ë‹¤ìŒ ìœ„ì¹˜ì— ì €ì¥ë¨:

[core]
fernet_key = <key_value>
ì´ í‚¤ê°€ ë³€ê²½ë˜ë©´ ì•”í˜¸í™”ëœ ê¸°ì¡´ ë³€ìˆ˜ëŠ” ë³µí˜¸í™”ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì ˆëŒ€ ì„ì˜ë¡œ ë°”ê¾¸ë©´ ì•ˆ ë¨.

ğŸ”§ ì§ì ‘ í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ë‹¤ë©´
Admin > Variables ì—ì„œ access_api_keyë¼ëŠ” ë³€ìˆ˜ ì¶”ê°€

ê°’: 1234567890abcdef

ì €ì¥ í›„ ë‹¤ì‹œ ì—´ë©´ ê°’ì´ ********ì²˜ëŸ¼ ì•”í˜¸í™”ë˜ì–´ ë³´ì´ë©´ ì„±ê³µ

ğŸ“ ìš”ì•½
ì¡°ê±´	ì„¤ëª…
ì´ë¦„ì— í¬í•¨	secret ë˜ëŠ” access
íš¨ê³¼	ê°’ì´ Fernet ì•”í˜¸í™”ë˜ì–´ ì €ì¥ë¨
ëŒ€ì†Œë¬¸ì	êµ¬ë¶„í•˜ì§€ ì•ŠìŒ
UI í‘œì‹œ	******** í˜•íƒœ
í•„ìš”í•˜ë©´ Fernet í‚¤ ê´€ë¦¬ë‚˜ ë³µí˜¸í™” í…ŒìŠ¤íŠ¸ ë°©ë²•ë„ ì•Œë ¤ì¤„ ìˆ˜ ìˆì–´.

ë‚˜ì˜ ë§:
airflow.cfg íŒŒì¼ ë‚´ìš©ì„ ë³€ê²½í–ˆì„ë•Œ ì €ì¥í•˜ê³  ë‚˜ì™”ì„ë–„ ì–´ë–¤ ëª…ë ¹ì–´ë¥¼ í•´ì•¼ ì‹¤ì œë¡œ ë°˜ì˜ë˜ëŠ”ê±°ì•¼
ChatGPTì˜ ë§:
Airflowì—ì„œ airflow.cfg íŒŒì¼ì„ ìˆ˜ì •í•œ í›„, ë³€ê²½ ì‚¬í•­ì„ ì‹¤ì œë¡œ ë°˜ì˜í•˜ë ¤ë©´ ë°˜ë“œì‹œ Airflow í”„ë¡œì„¸ìŠ¤ë¥¼ ì¬ì‹œì‘í•´ì•¼ í•´.
ì¦‰, ì„¤ì •ì€ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ì— ìë™ ë°˜ì˜ë˜ì§€ ì•ŠëŠ”ë‹¤.

âœ… ë³€ê²½ì‚¬í•­ ë°˜ì˜ ë°©ë²•
1. ë„ì»¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
docker-compose restart
ë˜ëŠ” ì„ íƒì ìœ¼ë¡œ í•„ìš”í•œ ì„œë¹„ìŠ¤ë§Œ:

docker-compose restart webserver scheduler
ì´ ëª…ë ¹ì–´ëŠ” ì›¹ì„œë²„ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ ì»¨í…Œì´ë„ˆë¥¼ ì¬ì‹œì‘í•´ì„œ ë³€ê²½ëœ ì„¤ì •ì„ ë¡œë“œí•˜ê²Œ í•¨.

2. ë„ì»¤ ì—†ì´ ì§ì ‘ ì„¤ì¹˜ëœ Airflow (ì˜ˆ: ê°€ìƒí™˜ê²½)
ì›¹ì„œë²„ ì¬ì‹œì‘
airflow webserver --daemon
ìŠ¤ì¼€ì¤„ëŸ¬ ì¬ì‹œì‘
airflow scheduler --daemon
ë˜ëŠ” ì‹¤í–‰ ì¤‘ì´ë©´ ë¨¼ì € ì¢…ë£Œ:

pkill -f "airflow webserver"
pkill -f "airflow scheduler"
ê·¸ ë‹¤ìŒ ë‹¤ì‹œ ì‹¤í–‰.

ğŸ§ª ë°˜ì˜ ì—¬ë¶€ í™•ì¸
ë³€ê²½í•œ ì„¤ì •ì´ ì‹¤ì œ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ ì‚¬ìš©:

airflow config get-value SECTION KEY
ì˜ˆ:

airflow config get-value core dags_folder
ğŸ“ ìš”ì•½
í™˜ê²½	ë°˜ì˜ ë°©ë²•
Docker	docker-compose restart ë˜ëŠ” ê°œë³„ ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
ë¡œì»¬ ì‹¤í–‰	ì›¹ì„œë²„/ìŠ¤ì¼€ì¤„ëŸ¬ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ í›„ ì¬ì‹¤í–‰
ì ìš© í™•ì¸	airflow config get-value ëª…ë ¹ì–´ë¡œ í™•ì¸ ê°€ëŠ¥
docker-compose.yamlì´ë‚˜ ì–´ë–¤ í™˜ê²½ì¸ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ë©´ ë” ì •í™•í•˜ê²Œ ë°˜ì˜ ë°©ë²•ì„ ì•Œë ¤ì¤„ ìˆ˜ ìˆì–´.

ë‚˜ì˜ ë§:
airflowë¥¼ ì‚¬ìš©í•˜ëŠ” DBëŠ” Metadata db ì´ë‹¤. ë””í´íŠ¸ë¡œëŠ” sqliteê°€ ì„¤ì¹˜ëœë‹¤. ì •ìƒì ì¸ ìš´ì˜ì„ ìœ„í•´ì„œ postgres,Mysqlë“±ì„ ì‚¬ìš©í•´ì•¼í•œë‹¤. ì´ë•Œ ì´ DBë¥¼ ì•”í˜¸í™”í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” í•„ë“œëŠ” airflow.cfgì—ì„œ ì–´ë–¤ í•„ë“œì•¼?
ChatGPTì˜ ë§:
ì¢‹ì•„, ì§ˆë¬¸ì˜ í•µì‹¬ì€ ë‹¤ìŒê³¼ ê°™ì•„:

Airflowê°€ ì‚¬ìš©í•˜ëŠ” Metadata DB ìì²´ë¥¼ ì•”í˜¸í™”í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ì„¤ì • ê°’ì€ ë¬´ì—‡ì¸ê°€?

âœ… ì •ë‹µ: fernet_key (in [core] section of airflow.cfg)
[core]
fernet_key = <ì•”í˜¸í™” í‚¤ ë¬¸ìì—´>
ì´ í‚¤ëŠ” Airflowì—ì„œ ë¯¼ê°í•œ ë°ì´í„°ë¥¼ Metadata DBì— ì•”í˜¸í™”í•´ì„œ ì €ì¥í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ëŒ€ì¹­í‚¤ì•¼.

ğŸ” fernet_keyê°€ ì•”í˜¸í™”ì— ì‚¬ìš©í•˜ëŠ” ëŒ€ìƒë“¤:
Variables (ì´ë¦„ì— secret, access í¬í•¨)

Connections (ë¹„ë°€ë²ˆí˜¸, í† í°, í‚¤ ë“±)

XCom ê°’ (í° JSON ê°ì²´, API ì‘ë‹µ ë“±)

ê¸°íƒ€ ì•”í˜¸í™”ëœ ë¯¼ê° ì •ë³´

âŒ ì˜¤í•´ ì£¼ì˜
fernet_keyëŠ” Metadata DB ì „ì²´ë¥¼ ì•”í˜¸í™”í•˜ëŠ” í‚¤ê°€ ì•„ë‹˜.

DB ìì²´ì˜ íŒŒì¼ì´ë‚˜ í…Œì´ë¸”ì´ ì•”í˜¸í™”ë˜ëŠ” ê²Œ ì•„ë‹ˆë¼,
DBì— ì €ì¥ë˜ëŠ” ë¯¼ê°í•œ â€œê°’â€ ë“¤ë§Œ ì•”í˜¸í™”ë¨.

âœ… fernet_key í™•ì¸ ë° ì„¤ì •
ìë™ ìƒì„± (ì´ˆê¸° ì„¤ì¹˜ ì‹œ)
airflow config get-value core fernet_key
ë˜ëŠ” ìˆ˜ë™ ìƒì„±:

python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
ì´ ê°’ì„ airflow.cfgì— ë¶™ì—¬ ë„£ìŒ:

[core]
fernet_key = DUSljqgV8zPYnYy7fE5zVftAwE6bh3iV0V07-ZHJlVg=
ğŸ›‘ ì£¼ì˜
fernet_keyëŠ” ì ˆëŒ€ë¡œ ì¤‘ê°„ì— ë°”ê¾¸ë©´ ì•ˆ ë¨
â†’ ê¸°ì¡´ì— ì•”í˜¸í™”ëœ ê°’ë“¤ì´ ë³µí˜¸í™”ë˜ì§€ ì•ŠìŒ â†’ ì—ëŸ¬ ë°œìƒ

ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ì´ í‚¤ë¥¼ ì•ˆì „í•œ ì¥ì†Œ(ì˜ˆ: Vault, AWS Secrets Manager ë“±)ì— ì €ì¥í•´ì•¼ í•¨

âœ… ìš´ì˜ìš© DB(PostgreSQL, MySQL) ì„¤ì •ì€ ë”°ë¡œ ìˆìŒ
[database]
sql_alchemy_conn = postgresql+psycopg2://user:password@host:5432/airflow_db
sql_alchemy_connì€ DB ì—°ê²° ì •ë³´,
fernet_keyëŠ” ê·¸ DB ì•ˆì— ë“¤ì–´ê°ˆ ë¯¼ê°í•œ ê°’ ì•”í˜¸í™”ìš© í‚¤.

ğŸ“ ìš”ì•½
í•­ëª©	í‚¤	ì„¤ëª…
ë¯¼ê° ë°ì´í„° ì•”í˜¸í™” í‚¤	fernet_key	Variables, Connections ë“± ë¯¼ê° ê°’ ì•”í˜¸í™”ì— ì‚¬ìš©
DB ì—°ê²°	sql_alchemy_conn	Metadata DB(Postgres, MySQL ë“±) ì—°ê²° ì •ë³´ ì„¤ì •
ì „ì²´ DB ì•”í˜¸í™”?	âœ–ï¸	í…Œì´ë¸”/íŒŒì¼ ë‹¨ìœ„ ì•”í˜¸í™”ëŠ” ë³„ë„ DB ìˆ˜ì¤€ì—ì„œ êµ¬ì„±í•´ì•¼ í•¨
í•„ìš”í•˜ë‹¤ë©´ PostgreSQL ë“±ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ë°©ë²•ë„ ì•ˆë‚´í•´ì¤„ ìˆ˜ ìˆì–´.

ë‚˜ì˜ ë§:
[2025-05-26T07:38:32.621+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/Gsheet_to_Redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/Gsheet_to_Redshift.py", line 41, in <module>
    from plugins import gsheet
  File "/opt/airflow/dags/plugins/gsheet.py", line 4, in <module>
    from oauth2client.service_account import ServiceAccountCredentials
ModuleNotFoundError: No module named 'oauth2client'
SELECT * FROM prod.nps WHERE DATE(created_at) = DATE('{{ execution_date }}')
[2025-05-26T07:38:32.749+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/SQL_to_Sheet.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/SQL_to_Sheet.py", line 4, in <module>
    from plugins import gsheet
  File "/opt/airflow/dags/plugins/gsheet.py", line 4, in <module>
    from oauth2client.service_account import ServiceAccountCredentials
ModuleNotFoundError: No module named 'oauth2client'
[2025-05-26T07:38:33.482+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/Weather_to_Redshift_v2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/Weather_to_Redshift_v2.py", line 98, in <module>
    etl("keeyong", "weather_forecast_v2", 37.5665, 126.9780, Variable.get("open_weather_api_key"))
                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable open_weather_api_key does not exist'
/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py:351 RemovedInAirflow3Warning: This decorator is deprecated.

In previous versions, all subclasses of BaseOperator must use apply_default decorator for the default_args feature to work properly.

In current version, it is optional. The decorator is applied automatically using the metaclass.

[2025-05-26T07:38:33.781+0000] {dag.py:4206} INFO - dagrun id: UpdateSymbol
[2025-05-26T07:38:33.790+0000] {dag.py:4222} INFO - created dagrun <DagRun UpdateSymbol @ 2023-05-30 00:00:00+00:00: manual__2023-05-30T00:00:00+00:00, state:running, queued_at: None. externally triggered: False>
[2025-05-26T07:38:33.811+0000] {dag.py:4168} INFO - [DAG TEST] starting task_id=get_historical_prices map_index=-1
[2025-05-26T07:38:33.812+0000] {dag.py:4171} INFO - [DAG TEST] running task <TaskInstance: UpdateSymbol.get_historical_prices manual__2023-05-30T00:00:00+00:00 [scheduled]>
[2025-05-26 07:38:34,032] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='UpdateSymbol' AIRFLOW_CTX_TASK_ID='get_historical_prices' AIRFLOW_CTX_EXECUTION_DATE='2023-05-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-05-30T00:00:00+00:00'
[2025-05-26T07:38:34.032+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='UpdateSymbol' AIRFLOW_CTX_TASK_ID='get_historical_prices' AIRFLOW_CTX_EXECUTION_DATE='2023-05-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-05-30T00:00:00+00:00'
[2025-05-26T07:38:34.034+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-05-26 07:38:35,465] {python.py:237} INFO - Done. Returned value was: [['2025-04-24 00:00:00', 204.62166872266098, 208.5565112075726, 202.67422555441442, 208.09710693359375, 47311000.0], ['2025-04-25 00:00:00', 206.0997277622984, 209.47530610539675, 205.92995222724628, 209.00592041015625, 38222300.0], ['2025-04-28 00:00:00', 209.72497582911666, 211.2230113707532, 207.1883090170199, 209.8647918701172, 38743100.0], ['2025-04-29 00:00:00', 208.41669143858087, 211.96204522644123, 208.09710321217068, 210.9333953857422, 36827600.0], ['2025-04-30 00:00:00', 209.02590396975114, 213.30029766677714, 206.399343338795, 212.22171020507812, 52286500.0], ['2025-05-01 00:00:00', 208.80618155790114, 214.2790004444518, 208.62640936955898, 213.04063415527344, 57365700.0], ['2025-05-02 00:00:00', 205.82009105417706, 206.71892151455646, 201.89524528415058, 205.08106994628906, 101010600.0], ['2025-05-05 00:00:00', 202.83402512431957, 203.83271551835597, 197.9504297070337, 198.62953186035156, 69018500.0], ['2025-05-06 00:00:00', 197.9504203457911, 200.38721199140494, 196.7619763948813, 198.25001525878906, 51216500.0], ['2025-05-07 00:00:00', 198.90915494694545, 199.1788056079939, 192.99690991055434, 195.99298095703125, 68536700.0], ['2025-05-08 00:00:00', 197.46106353953903, 199.78801395815432, 194.42503624466005, 197.2313690185547, 50478900.0], ['2025-05-09 00:00:00', 198.739389980464, 200.2773664943336, 197.28129528859796, 198.27000427246094, 36453900.0], ['2025-05-12 00:00:00', 210.97000122070312, 211.27000427246094, 206.75, 210.7899932861328, 63775800.0], ['2025-05-13 00:00:00', 210.42999267578125, 213.39999389648438, 209.0, 212.92999267578125, 51909300.0], ['2025-05-14 00:00:00', 212.42999267578125, 213.94000244140625, 210.5800018310547, 212.3300018310547, 49325800.0], ['2025-05-15 00:00:00', 210.9499969482422, 212.9600067138672, 209.5399932861328, 211.4499969482422, 45029500.0], ['2025-05-16 00:00:00', 212.36000061035156, 212.57000732421875, 209.77000427246094, 211.25999450683594, 54737900.0], ['2025-05-19 00:00:00', 207.91000366210938, 209.47999572753906, 204.25999450683594, 208.77999877929688, 46140500.0], ['2025-05-20 00:00:00', 207.6699981689453, 208.47000122070312, 205.02999877929688, 206.86000061035156, 42496600.0], ['2025-05-21 00:00:00', 205.1699981689453, 207.0399932861328, 200.7100067138672, 202.08999633789062, 59211800.0], ['2025-05-22 00:00:00', 200.7100067138672, 202.75, 199.6999969482422, 201.36000061035156, 46742400.0], ['2025-05-23 00:00:00', 193.6699981689453, 197.6999969482422, 193.4600067138672, 195.27000427246094, 78314800.0]]
[2025-05-26T07:38:35.465+0000] {python.py:237} INFO - Done. Returned value was: [['2025-04-24 00:00:00', 204.62166872266098, 208.5565112075726, 202.67422555441442, 208.09710693359375, 47311000.0], ['2025-04-25 00:00:00', 206.0997277622984, 209.47530610539675, 205.92995222724628, 209.00592041015625, 38222300.0], ['2025-04-28 00:00:00', 209.72497582911666, 211.2230113707532, 207.1883090170199, 209.8647918701172, 38743100.0], ['2025-04-29 00:00:00', 208.41669143858087, 211.96204522644123, 208.09710321217068, 210.9333953857422, 36827600.0], ['2025-04-30 00:00:00', 209.02590396975114, 213.30029766677714, 206.399343338795, 212.22171020507812, 52286500.0], ['2025-05-01 00:00:00', 208.80618155790114, 214.2790004444518, 208.62640936955898, 213.04063415527344, 57365700.0], ['2025-05-02 00:00:00', 205.82009105417706, 206.71892151455646, 201.89524528415058, 205.08106994628906, 101010600.0], ['2025-05-05 00:00:00', 202.83402512431957, 203.83271551835597, 197.9504297070337, 198.62953186035156, 69018500.0], ['2025-05-06 00:00:00', 197.9504203457911, 200.38721199140494, 196.7619763948813, 198.25001525878906, 51216500.0], ['2025-05-07 00:00:00', 198.90915494694545, 199.1788056079939, 192.99690991055434, 195.99298095703125, 68536700.0], ['2025-05-08 00:00:00', 197.46106353953903, 199.78801395815432, 194.42503624466005, 197.2313690185547, 50478900.0], ['2025-05-09 00:00:00', 198.739389980464, 200.2773664943336, 197.28129528859796, 198.27000427246094, 36453900.0], ['2025-05-12 00:00:00', 210.97000122070312, 211.27000427246094, 206.75, 210.7899932861328, 63775800.0], ['2025-05-13 00:00:00', 210.42999267578125, 213.39999389648438, 209.0, 212.92999267578125, 51909300.0], ['2025-05-14 00:00:00', 212.42999267578125, 213.94000244140625, 210.5800018310547, 212.3300018310547, 49325800.0], ['2025-05-15 00:00:00', 210.9499969482422, 212.9600067138672, 209.5399932861328, 211.4499969482422, 45029500.0], ['2025-05-16 00:00:00', 212.36000061035156, 212.57000732421875, 209.77000427246094, 211.25999450683594, 54737900.0], ['2025-05-19 00:00:00', 207.91000366210938, 209.47999572753906, 204.25999450683594, 208.77999877929688, 46140500.0], ['2025-05-20 00:00:00', 207.6699981689453, 208.47000122070312, 205.02999877929688, 206.86000061035156, 42496600.0], ['2025-05-21 00:00:00', 205.1699981689453, 207.0399932861328, 200.7100067138672, 202.08999633789062, 59211800.0], ['2025-05-22 00:00:00', 200.7100067138672, 202.75, 199.6999969482422, 201.36000061035156, 46742400.0], ['2025-05-23 00:00:00', 193.6699981689453, 197.6999969482422, 193.4600067138672, 195.27000427246094, 78314800.0]]
[2025-05-26T07:38:35.466+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-05-26T07:38:35.477+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=UpdateSymbol, task_id=get_historical_prices, run_id=manual__2023-05-30T00:00:00+00:00, execution_date=20230530T000000, start_date=, end_date=20250526T073835
[2025-05-26T07:38:35.483+0000] {dag.py:4182} INFO - [DAG TEST] end task task_id=get_historical_prices map_index=-1
[2025-05-26T07:38:35.492+0000] {dag.py:4168} INFO - [DAG TEST] starting task_id=load map_index=-1
[2025-05-26T07:38:35.492+0000] {dag.py:4171} INFO - [DAG TEST] running task <TaskInstance: UpdateSymbol.load manual__2023-05-30T00:00:00+00:00 [scheduled]>
[2025-05-26 07:38:35,512] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='UpdateSymbol' AIRFLOW_CTX_TASK_ID='load' AIRFLOW_CTX_EXECUTION_DATE='2023-05-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-05-30T00:00:00+00:00'
[2025-05-26T07:38:35.512+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='UpdateSymbol' AIRFLOW_CTX_TASK_ID='load' AIRFLOW_CTX_EXECUTION_DATE='2023-05-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-05-30T00:00:00+00:00'
[2025-05-26T07:38:35.513+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-05-26T07:38:35.513+0000] {UpdateSymbol.py:34} INFO - load started
[2025-05-26T07:38:35.516+0000] {base.py:84} INFO - Using connection ID 'redshift_dev_db' for task execution.
permission denied for schema keeyong

[2025-05-26T07:38:35.728+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-05-26 07:38:35,728] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 265, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/UpdateSymbol.py", line 38, in load
    cur.execute(f"DROP TABLE IF EXISTS {schema}.{table};")
psycopg2.errors.InsufficientPrivilege: permission denied for schema keeyong

[2025-05-26T07:38:35.728+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 265, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/UpdateSymbol.py", line 38, in load
    cur.execute(f"DROP TABLE IF EXISTS {schema}.{table};")
psycopg2.errors.InsufficientPrivilege: permission denied for schema keeyong

[2025-05-26T07:38:35.735+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=UpdateSymbol, task_id=load, run_id=manual__2023-05-30T00:00:00+00:00, execution_date=20230530T000000, start_date=, end_date=20250526T073835
[2025-05-26T07:38:35.741+0000] {dag.py:2932} ERROR - Task failed; ti=<TaskInstance: UpdateSymbol.load manual__2023-05-30T00:00:00+00:00 [failed]>
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 2930, in test
    _run_task(ti=ti, inline_trigger=not triggerer_running, session=session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 4172, in _run_task
    ti._run_raw_task(session=session, raise_on_defer=inline_trigger)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2479, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2676, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2701, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 265, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/UpdateSymbol.py", line 38, in load
    cur.execute(f"DROP TABLE IF EXISTS {schema}.{table};")
psycopg2.errors.InsufficientPrivilege: permission denied for schema keeyong

[2025-05-26T07:38:35.746+0000] {dagrun.py:820} ERROR - Marking run <DagRun UpdateSymbol @ 2023-05-30 00:00:00+00:00: manual__2023-05-30T00:00:00+00:00, state:running, queued_at: None. externally triggered: False> failed
[2025-05-26T07:38:35.746+0000] {dagrun.py:902} INFO - DagRun Finished: dag_id=UpdateSymbol, execution_date=2023-05-30 00:00:00+00:00, run_id=manual__2023-05-30T00:00:00+00:00, run_start_date=2023-05-30 00:00:00+00:00, run_end_date=2025-05-26 07:38:35.746845+00:00, run_duration=62840315.746845, state=failed, external_trigger=False, run_type=manual, data_interval_start=2023-05-28 10:00:00+00:00, data_interval_end=2023-05-29 10:00:00+00:00, dag_hash=None
DagRun failed
(airflow)airflow dags test UpdateSymbol 2025-05-26
[2025-05-26T07:39:05.095+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags
[2025-05-26T07:39:05.600+0000] {redshift_summary.py:48} INFO - /opt/airflow/dags
[2025-05-26T07:39:05.666+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/Gsheet_to_Redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/Gsheet_to_Redshift.py", line 41, in <module>
    from plugins import gsheet
  File "/opt/airflow/dags/plugins/gsheet.py", line 4, in <module>
    from oauth2client.service_account import ServiceAccountCredentials
ModuleNotFoundError: No module named 'oauth2client'
SELECT * FROM prod.nps WHERE DATE(created_at) = DATE('{{ execution_date }}')
[2025-05-26T07:39:05.788+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/SQL_to_Sheet.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/SQL_to_Sheet.py", line 4, in <module>
    from plugins import gsheet
  File "/opt/airflow/dags/plugins/gsheet.py", line 4, in <module>
    from oauth2client.service_account import ServiceAccountCredentials
ModuleNotFoundError: No module named 'oauth2client'
[2025-05-26T07:39:06.505+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/Weather_to_Redshift_v2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/Weather_to_Redshift_v2.py", line 98, in <module>
    etl("keeyong", "weather_forecast_v2", 37.5665, 126.9780, Variable.get("open_weather_api_key"))
                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable open_weather_api_key does not exist'
/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py:351 RemovedInAirflow3Warning: This decorator is deprecated.

In previous versions, all subclasses of BaseOperator must use apply_default decorator for the default_args feature to work properly.

In current version, it is optional. The decorator is applied automatically using the metaclass.

[2025-05-26T07:39:06.870+0000] {dag.py:4206} INFO - dagrun id: UpdateSymbol
[2025-05-26T07:39:06.882+0000] {dag.py:4222} INFO - created dagrun <DagRun UpdateSymbol @ 2025-05-26 00:00:00+00:00: manual__2025-05-26T00:00:00+00:00, state:running, queued_at: None. externally triggered: False>
[2025-05-26T07:39:06.901+0000] {dag.py:4168} INFO - [DAG TEST] starting task_id=get_historical_prices map_index=-1
[2025-05-26T07:39:06.902+0000] {dag.py:4171} INFO - [DAG TEST] running task <TaskInstance: UpdateSymbol.get_historical_prices manual__2025-05-26T00:00:00+00:00 [scheduled]>
[2025-05-26 07:39:07,107] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='UpdateSymbol' AIRFLOW_CTX_TASK_ID='get_historical_prices' AIRFLOW_CTX_EXECUTION_DATE='2025-05-26T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-26T00:00:00+00:00'
[2025-05-26T07:39:07.107+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='UpdateSymbol' AIRFLOW_CTX_TASK_ID='get_historical_prices' AIRFLOW_CTX_EXECUTION_DATE='2025-05-26T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-26T00:00:00+00:00'
[2025-05-26T07:39:07.109+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-05-26 07:39:07,754] {python.py:237} INFO - Done. Returned value was: [['2025-04-24 00:00:00', 204.62166872266098, 208.5565112075726, 202.67422555441442, 208.09710693359375, 47311000.0], ['2025-04-25 00:00:00', 206.0997277622984, 209.47530610539675, 205.92995222724628, 209.00592041015625, 38222300.0], ['2025-04-28 00:00:00', 209.72497582911666, 211.2230113707532, 207.1883090170199, 209.8647918701172, 38743100.0], ['2025-04-29 00:00:00', 208.41669143858087, 211.96204522644123, 208.09710321217068, 210.9333953857422, 36827600.0], ['2025-04-30 00:00:00', 209.02590396975114, 213.30029766677714, 206.399343338795, 212.22171020507812, 52286500.0], ['2025-05-01 00:00:00', 208.80618155790114, 214.2790004444518, 208.62640936955898, 213.04063415527344, 57365700.0], ['2025-05-02 00:00:00', 205.82009105417706, 206.71892151455646, 201.89524528415058, 205.08106994628906, 101010600.0], ['2025-05-05 00:00:00', 202.83402512431957, 203.83271551835597, 197.9504297070337, 198.62953186035156, 69018500.0], ['2025-05-06 00:00:00', 197.9504203457911, 200.38721199140494, 196.7619763948813, 198.25001525878906, 51216500.0], ['2025-05-07 00:00:00', 198.90915494694545, 199.1788056079939, 192.99690991055434, 195.99298095703125, 68536700.0], ['2025-05-08 00:00:00', 197.46106353953903, 199.78801395815432, 194.42503624466005, 197.2313690185547, 50478900.0], ['2025-05-09 00:00:00', 198.739389980464, 200.2773664943336, 197.28129528859796, 198.27000427246094, 36453900.0], ['2025-05-12 00:00:00', 210.97000122070312, 211.27000427246094, 206.75, 210.7899932861328, 63775800.0], ['2025-05-13 00:00:00', 210.42999267578125, 213.39999389648438, 209.0, 212.92999267578125, 51909300.0], ['2025-05-14 00:00:00', 212.42999267578125, 213.94000244140625, 210.5800018310547, 212.3300018310547, 49325800.0], ['2025-05-15 00:00:00', 210.9499969482422, 212.9600067138672, 209.5399932861328, 211.4499969482422, 45029500.0], ['2025-05-16 00:00:00', 212.36000061035156, 212.57000732421875, 209.77000427246094, 211.25999450683594, 54737900.0], ['2025-05-19 00:00:00', 207.91000366210938, 209.47999572753906, 204.25999450683594, 208.77999877929688, 46140500.0], ['2025-05-20 00:00:00', 207.6699981689453, 208.47000122070312, 205.02999877929688, 206.86000061035156, 42496600.0], ['2025-05-21 00:00:00', 205.1699981689453, 207.0399932861328, 200.7100067138672, 202.08999633789062, 59211800.0], ['2025-05-22 00:00:00', 200.7100067138672, 202.75, 199.6999969482422, 201.36000061035156, 46742400.0], ['2025-05-23 00:00:00', 193.6699981689453, 197.6999969482422, 193.4600067138672, 195.27000427246094, 78314800.0]]
[2025-05-26T07:39:07.754+0000] {python.py:237} INFO - Done. Returned value was: [['2025-04-24 00:00:00', 204.62166872266098, 208.5565112075726, 202.67422555441442, 208.09710693359375, 47311000.0], ['2025-04-25 00:00:00', 206.0997277622984, 209.47530610539675, 205.92995222724628, 209.00592041015625, 38222300.0], ['2025-04-28 00:00:00', 209.72497582911666, 211.2230113707532, 207.1883090170199, 209.8647918701172, 38743100.0], ['2025-04-29 00:00:00', 208.41669143858087, 211.96204522644123, 208.09710321217068, 210.9333953857422, 36827600.0], ['2025-04-30 00:00:00', 209.02590396975114, 213.30029766677714, 206.399343338795, 212.22171020507812, 52286500.0], ['2025-05-01 00:00:00', 208.80618155790114, 214.2790004444518, 208.62640936955898, 213.04063415527344, 57365700.0], ['2025-05-02 00:00:00', 205.82009105417706, 206.71892151455646, 201.89524528415058, 205.08106994628906, 101010600.0], ['2025-05-05 00:00:00', 202.83402512431957, 203.83271551835597, 197.9504297070337, 198.62953186035156, 69018500.0], ['2025-05-06 00:00:00', 197.9504203457911, 200.38721199140494, 196.7619763948813, 198.25001525878906, 51216500.0], ['2025-05-07 00:00:00', 198.90915494694545, 199.1788056079939, 192.99690991055434, 195.99298095703125, 68536700.0], ['2025-05-08 00:00:00', 197.46106353953903, 199.78801395815432, 194.42503624466005, 197.2313690185547, 50478900.0], ['2025-05-09 00:00:00', 198.739389980464, 200.2773664943336, 197.28129528859796, 198.27000427246094, 36453900.0], ['2025-05-12 00:00:00', 210.97000122070312, 211.27000427246094, 206.75, 210.7899932861328, 63775800.0], ['2025-05-13 00:00:00', 210.42999267578125, 213.39999389648438, 209.0, 212.92999267578125, 51909300.0], ['2025-05-14 00:00:00', 212.42999267578125, 213.94000244140625, 210.5800018310547, 212.3300018310547, 49325800.0], ['2025-05-15 00:00:00', 210.9499969482422, 212.9600067138672, 209.5399932861328, 211.4499969482422, 45029500.0], ['2025-05-16 00:00:00', 212.36000061035156, 212.57000732421875, 209.77000427246094, 211.25999450683594, 54737900.0], ['2025-05-19 00:00:00', 207.91000366210938, 209.47999572753906, 204.25999450683594, 208.77999877929688, 46140500.0], ['2025-05-20 00:00:00', 207.6699981689453, 208.47000122070312, 205.02999877929688, 206.86000061035156, 42496600.0], ['2025-05-21 00:00:00', 205.1699981689453, 207.0399932861328, 200.7100067138672, 202.08999633789062, 59211800.0], ['2025-05-22 00:00:00', 200.7100067138672, 202.75, 199.6999969482422, 201.36000061035156, 46742400.0], ['2025-05-23 00:00:00', 193.6699981689453, 197.6999969482422, 193.4600067138672, 195.27000427246094, 78314800.0]]
[2025-05-26T07:39:07.755+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-05-26T07:39:07.763+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=UpdateSymbol, task_id=get_historical_prices, run_id=manual__2025-05-26T00:00:00+00:00, execution_date=20250526T000000, start_date=, end_date=20250526T073907
[2025-05-26T07:39:07.767+0000] {dag.py:4182} INFO - [DAG TEST] end task task_id=get_historical_prices map_index=-1
[2025-05-26T07:39:07.774+0000] {dag.py:4168} INFO - [DAG TEST] starting task_id=load map_index=-1
[2025-05-26T07:39:07.774+0000] {dag.py:4171} INFO - [DAG TEST] running task <TaskInstance: UpdateSymbol.load manual__2025-05-26T00:00:00+00:00 [scheduled]>
[2025-05-26 07:39:07,790] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='UpdateSymbol' AIRFLOW_CTX_TASK_ID='load' AIRFLOW_CTX_EXECUTION_DATE='2025-05-26T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-26T00:00:00+00:00'
[2025-05-26T07:39:07.790+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='UpdateSymbol' AIRFLOW_CTX_TASK_ID='load' AIRFLOW_CTX_EXECUTION_DATE='2025-05-26T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-26T00:00:00+00:00'
[2025-05-26T07:39:07.791+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-05-26T07:39:07.791+0000] {UpdateSymbol.py:34} INFO - load started
[2025-05-26T07:39:07.794+0000] {base.py:84} INFO - Using connection ID 'redshift_dev_db' for task execution.
permission denied for schema keeyong

[2025-05-26T07:39:08.026+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-05-26 07:39:08,027] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 265, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/UpdateSymbol.py", line 38, in load
    cur.execute(f"DROP TABLE IF EXISTS {schema}.{table};")
psycopg2.errors.InsufficientPrivilege: permission denied for schema keeyong

[2025-05-26T07:39:08.027+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 265, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/UpdateSymbol.py", line 38, in load
    cur.execute(f"DROP TABLE IF EXISTS {schema}.{table};")
psycopg2.errors.InsufficientPrivilege: permission denied for schema keeyong

[2025-05-26T07:39:08.032+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=UpdateSymbol, task_id=load, run_id=manual__2025-05-26T00:00:00+00:00, execution_date=20250526T000000, start_date=, end_date=20250526T073908
[2025-05-26T07:39:08.037+0000] {dag.py:2932} ERROR - Task failed; ti=<TaskInstance: UpdateSymbol.load manual__2025-05-26T00:00:00+00:00 [failed]>
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 2930, in test
    _run_task(ti=ti, inline_trigger=not triggerer_running, session=session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dag.py", line 4172, in _run_task
    ti._run_raw_task(session=session, raise_on_defer=inline_trigger)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2479, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2676, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2701, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 265, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/UpdateSymbol.py", line 38, in load
    cur.execute(f"DROP TABLE IF EXISTS {schema}.{table};")
psycopg2.errors.InsufficientPrivilege: permission denied for schema keeyong

[2025-05-26T07:39:08.041+0000] {dagrun.py:820} ERROR - Marking run <DagRun UpdateSymbol @ 2025-05-26 00:00:00+00:00: manual__2025-05-26T00:00:00+00:00, state:running, queued_at: None. externally triggered: False> failed
ChatGPTì˜ ë§:
ì—ëŸ¬ ë¡œê·¸ë¥¼ ì •ë¦¬í•´ì„œ ë¬¸ì œë¥¼ ì„¸ ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆì–´. ê° ë¬¸ì œì˜ ì›ì¸ê³¼ í•´ê²° ë°©ë²•ì„ ì •í™•íˆ ì§šì–´ì¤„ê²Œ.

ğŸš¨ 1. ModuleNotFoundError: No module named 'oauth2client'
ğŸ” ì›ì¸
gsheet.pyì—ì„œ from oauth2client.service_account import ServiceAccountCredentialsë¥¼ ì“°ê³  ìˆëŠ”ë°,
ë„ì»¤ í™˜ê²½ì— oauth2client íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ì„œ ë°œìƒ.

âœ… í•´ê²° ë°©ë²•
ë„ì»¤ ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰:

pip install oauth2client
ë˜ëŠ” Dockerfile ë˜ëŠ” docker-compose.yamlì— requirements.txtë¥¼ ëª…ì‹œí•´ ì•„ë˜ì²˜ëŸ¼ ì²˜ë¦¬:

# Dockerfile ì˜ˆì‹œ
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
requirements.txtì— ì¶”ê°€:

oauth2client
ì„¤ì • ë³€ê²½ í›„ ë„ì»¤ ì¬ë¹Œë“œ:

docker-compose build
docker-compose up -d
ğŸš¨ 2. KeyError: 'Variable open_weather_api_key does not exist'
ğŸ” ì›ì¸
DAG ì½”ë“œì—ì„œ Variable.get("open_weather_api_key")ë¥¼ í˜¸ì¶œí–ˆì§€ë§Œ, í•´ë‹¹ ë³€ìˆ˜ê°€ Airflow Admin UI > Variablesì— ì¡´ì¬í•˜ì§€ ì•ŠìŒ.

âœ… í•´ê²° ë°©ë²•
Airflow UIì—ì„œ ë‹¤ìŒ ê²½ë¡œë¡œ ì´ë™í•´ ë³€ìˆ˜ ë“±ë¡:

Admin > Variables > + (Create)
Key: open_weather_api_key  
Value: <ì‹¤ì œ API í‚¤>
ë˜ëŠ” CLIë¡œë„ ê°€ëŠ¥:

airflow variables set open_weather_api_key "YOUR_API_KEY"
ğŸš¨ 3. psycopg2.errors.InsufficientPrivilege: permission denied for schema keeyong
ğŸ” ì›ì¸
Redshiftì— ì—°ê²°ëœ ì‚¬ìš©ìê°€ keeyong ìŠ¤í‚¤ë§ˆì— ëŒ€í•´ DROP TABLE, CREATE TABLE, ë˜ëŠ” INSERT ë“±ì˜ ê¶Œí•œì´ ì—†ìŒ.

âœ… í•´ê²° ë°©ë²•
Redshiftì— ë¡œê·¸ì¸í•´ì„œ ê¶Œí•œ ë¶€ì—¬:

GRANT USAGE ON SCHEMA keeyong TO your_user;
GRANT CREATE, SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA keeyong TO your_user;
í•„ìš”í•˜ë‹¤ë©´ ì•ìœ¼ë¡œ ìƒì„±ë  í…Œì´ë¸”ì—ë„ ìë™ ê¶Œí•œ ë¶€ì—¬:

ALTER DEFAULT PRIVILEGES IN SCHEMA keeyong GRANT ALL ON TABLES TO your_user;
your_userëŠ” Airflowì—ì„œ Redshiftì— ì ‘ì†í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ê³„ì •ëª…ê³¼ ë™ì¼í•´ì•¼ í•¨.

ğŸ“Œ ì •ë¦¬ ìš”ì•½
ë¬¸ì œ	ì›ì¸	í•´ê²° ë°©ë²•
No module named 'oauth2client'	íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜	pip install oauth2client ë˜ëŠ” requirements.txt
KeyError: Variable does not exist	Airflow ë³€ìˆ˜ ëˆ„ë½	Admin UI ë˜ëŠ” CLIë¡œ ë³€ìˆ˜ ë“±ë¡
permission denied for schema keeyong	DB ê³„ì • ê¶Œí•œ ë¶€ì¡±	Redshiftì—ì„œ GRANTë¡œ ê¶Œí•œ ë¶€ì—¬
í•„ìš”í•˜ë©´ docker-compose.yaml, DAG ì¼ë¶€, requirements.txt íŒŒì¼ë„ ê°™ì´ ê²€í† í•´ì¤„ ìˆ˜ ìˆì–´.
ìˆ˜ì •í•  ì¤€ë¹„ë˜ì—ˆìœ¼ë©´ ë¶™ì—¬ì¤˜.

ë‚˜ì˜ ë§:
pip install oauth2client  ì„¤ì¹˜í•˜ê³  build í•´ì•¼í•´? ì•„ë‹ˆë©´ .txt ë°©ë²•ìœ¼ë¡œí–ˆì„ë•Œ buildí•˜ëŠ”ê±°ì•¼
ChatGPTì˜ ë§:
ì •í™•íˆ ì§šê³  ê°ˆê²Œ.

âœ… ë‘ ë°©ë²•ì˜ ì°¨ì´
â‘  pip install oauth2client (ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ ì§ì ‘ ì„¤ì¹˜)
ë„ì»¤ ì»¨í…Œì´ë„ˆ ì•ˆì— ì¦‰ì‹œ ì„¤ì¹˜ë¨

ì§€ê¸ˆ ë‹¹ì¥ì€ ì‘ë™í•˜ì§€ë§Œ, ë„ì»¤ ì»¨í…Œì´ë„ˆë¥¼ ë‹¤ì‹œ ì‹œì‘í•˜ê±°ë‚˜ ìƒˆë¡œ ë¹Œë“œí•˜ë©´ ì„¤ì¹˜ ë‚´ìš©ì´ ì‚¬ë¼ì§

ì¦‰, ì„ì‹œ í•´ê²°ìš©

â‘¡ requirements.txt + Dockerfile or docker-compose â†’ build
ì´ ë°©ë²•ì€ ë„ì»¤ ì´ë¯¸ì§€ì— ì˜êµ¬ ë°˜ì˜ë¨

ì§€ì† ê°€ëŠ¥í•œ ë°©ì‹ì´ë©°, ì»¨í…Œì´ë„ˆê°€ ì¬ì‹œì‘ë˜ê±°ë‚˜ ë°°í¬ë˜ì–´ë„ ê³„ì† ìœ ì§€ë¨

buildëŠ” ì´ë•Œ ë°˜ë“œì‹œ í•„ìš”í•¨

ğŸš€ ì¶”ì²œ ë°©ì‹ (ì§€ì†ì  ìš´ì˜ìš©)
requirements.txtì— ì¶”ê°€:

oauth2client
docker-compose.yamlì— build: ë˜ëŠ” Dockerfile ì„¤ì • í™•ì¸

ì˜ˆì‹œ:

airflow-webserver:
  build:
    context: .
    dockerfile: Dockerfile
  volumes:
    - ./dags:/opt/airflow/dags
    - ./requirements.txt:/requirements.txt
  command: >
    bash -c "pip install -r /requirements.txt && airflow webserver"
ë¹Œë“œ ë° ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘:

docker-compose build
docker-compose up -d
âœ… ì–¸ì œ buildê°€ í•„ìš”í•˜ëƒ?
ì‘ì—…	build í•„ìš” ì—¬ë¶€
ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ pip installë§Œ í•œ ê²½ìš°	âŒ (í•˜ì§€ë§Œ ì¬ì‹œì‘ ì‹œ ë‚ ì•„ê°)
requirements.txtì— íŒ¨í‚¤ì§€ë¥¼ ì¶”ê°€í•œ ê²½ìš°	âœ… ë°˜ë“œì‹œ build í•„ìš”
Dockerfileì„ ìˆ˜ì •í•œ ê²½ìš°	âœ… ë°˜ë“œì‹œ build í•„ìš”
ğŸ“ ê²°ë¡ 
ê°œë°œ ì¤‘ í…ŒìŠ¤íŠ¸ìš©ì´ë©´: docker exec -it <container> pip install oauth2client

ìš´ì˜/ë°°í¬ í™˜ê²½ì´ë©´: requirements.txt â†’ build â†’ up -d

ì›í•˜ëŠ” ë°©ì‹ì— ë”°ë¼ ë„ì™€ì¤„ê²Œ. í˜„ì¬ ìš´ì˜ ë°©ì‹ì— ë§ì¶° ì„ íƒí•´.

ë‚˜ì˜ ë§:
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.
#
# WARNING: This configuration is for local development. Do not use it in a production deployment.
#
# This configuration supports basic configuration using environment variables or an .env file
# The following variables are supported:
#
# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.
#                                Default: apache/airflow:2.9.1
# AIRFLOW_UID                  - User ID in Airflow containers
#                                Default: 50000
# AIRFLOW_PROJ_DIR             - Base path to which all the files will be volumed.
#                                Default: .
# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode
#
# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).
#                                Default: airflow
# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).
#                                Default: airflow
# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.
#                                Use this option ONLY for quick checks. Installing requirements at container
#                                startup is done EVERY TIME the service is started.
#                                A better way is to build a custom image or extend the official image
#                                as described in https://airflow.apache.org/docs/docker-stack/build.html.
#                                Default: ''
#
# Feel free to modify this file to suit your needs.
---
x-airflow-common:
  &airflow-common
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the "build" line below, Then run docker-compose build to build the images.
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.1}
  # build: .
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    # yamllint disable rule:line-length
    # Use simple http server on scheduler for health checks
    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
    # yamllint enable rule:line-length
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
    # for other purpose (development, test and especially production usage) build/extend Airflow image.
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    # The following line can be used to set a custom config file, stored in the local config folder
    # If you want to use it, outcomment it and replace airflow.cfg with the name of your config file
    # AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  redis:
    # Redis is limited to 7.2-bookworm due to licencing change
    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
    image: redis:7.2-bookworm
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow

  # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
  # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # See: https://docs.docker.com/compose/profiles/
  flower:
    <<: *airflow-common
    command: celery flower
    profiles:
      - flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

volumes:
  postgres-db-volume:
ì´ê²Œ docker-compose.yaml íŒŒì¼ì¸ë° ì–´ë””ë¥¼ ìˆ˜ì •í•´
ChatGPTì˜ ë§:
ì¢‹ì•„. ë„¤ê°€ í•˜ê³ ì í•˜ëŠ” ì‘ì—…ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì¼ ê±°ì•¼:

ğŸ¯ ëª©í‘œ: oauth2client íŒ¨í‚¤ì§€ë¥¼ ëª¨ë“  Airflow ì„œë¹„ìŠ¤ ì»¨í…Œì´ë„ˆì— ë°˜ì˜í•˜ë ¤ê³  í•¨
í˜„ì¬ docker-compose.yamlì—ëŠ” _PIP_ADDITIONAL_REQUIREMENTS í™˜ê²½ë³€ìˆ˜ë¡œ ì¶”ê°€ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•  ìˆ˜ ìˆëŠ” ì„¤ì •ì´ ì´ë¯¸ ì¤€ë¹„ë¼ ìˆì–´. ë”°ë¼ì„œ ë³„ë„ì˜ build ì—†ì´ë„ ì‘ë™ ê°€ëŠ¥í•´.

âœ… ë°©ë²• 1: ê°„ë‹¨í•˜ê²Œ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì¹˜ (ì¶”ì²œ)
1. .env íŒŒì¼ì„ ë£¨íŠ¸ì— ìƒì„±í•˜ê³  ë‹¤ìŒ ë‚´ìš© ì¶”ê°€:
_PIP_ADDITIONAL_REQUIREMENTS=oauth2client
ì´ë¯¸ .env íŒŒì¼ì´ ìˆë‹¤ë©´ ê·¸ ì•ˆì— ì´ ë¼ì¸ë§Œ ì¶”ê°€í•´ë„ ë¼.

2. ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬ í›„ ì¬ì‹œì‘
docker-compose down
docker-compose up -d
Airflow ì»¨í…Œì´ë„ˆê°€ ë¶€íŒ… ì‹œë§ˆë‹¤ pip install oauth2clientë¥¼ ì‹¤í–‰í•˜ê²Œ ë¨.

âš ï¸ ë‹¨ì 
ì»¨í…Œì´ë„ˆê°€ ì¬ì‹œì‘ë  ë•Œë§ˆë‹¤ ì„¤ì¹˜ ì‹œê°„ì´ ê±¸ë¦¼

ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ë¹„íš¨ìœ¨ (â†’ ë‹¤ìŒ ë°©ë²• ì°¸ê³ )

âœ… ë°©ë²• 2: Docker ì´ë¯¸ì§€ì— ì•„ì˜ˆ í¬í•¨ì‹œí‚¤ê¸° (ìš´ì˜/ì§€ì†ìš©)
1. Dockerfile ìƒì„± (docker-compose.yamlì´ ìˆëŠ” í´ë”ì—)
FROM apache/airflow:2.9.1
USER root
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt
USER airflow
2. requirements.txt ìƒì„±
oauth2client
3. docker-compose.yaml ìˆ˜ì •
ê¸°ì¡´ì˜

image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.1}
ë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ê³  buildë¥¼ ì£¼ì„ í•´ì œ:

# image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.1}
build: .
4. ë¹Œë“œ & ì‹¤í–‰
docker-compose build
docker-compose up -d
ì´ì œ oauth2clientëŠ” ì´ë¯¸ì§€ì— í¬í•¨ë˜ì–´ ì¬ì‹œì‘í•´ë„ ê·¸ëŒ€ë¡œ ìœ ì§€ë¨.

ğŸ“ ìš”ì•½
ëª©ì 	ë°©ë²•	ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤
í…ŒìŠ¤íŠ¸, ë¡œì»¬ ê°œë°œ	.envì— _PIP_ADDITIONAL_REQUIREMENTS=oauth2client ì¶”ê°€	ë¹ ë¥´ê²Œ ì‹¤í—˜í•  ë•Œ
ìš´ì˜, ì¬ì‚¬ìš© ëª©ì 	Dockerfile + build: .	ì¥ê¸°ì , ì•ˆì •ì  ìš´ì˜ í•„ìš” ì‹œ
ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ê°ˆì§€ ì •í–ˆìœ¼ë©´ ë‚´ê°€ ë„ì™€ì¤„ê²Œ.
ì˜ˆë¥¼ ë“¤ì–´ .env ë§Œë“¤ì§€ ì•Šê³  .yamlì— ì§ì ‘ ì“°ê³  ì‹¶ë‹¤ë“ ì§€, ë‹¤ ê°€ëŠ¥í•´.

ë‚˜ì˜ ë§:
psycopg2.errors.InsufficientPrivilege: permission denied for schema keeyong 
ì–´ëŠ ë¶€ë¶„ì—ì„œ keeyongì— ì ‘ì†í•˜ë ¤ê³ í•´? ì½”ë“œì—ì„œ ëª»ì°¾ê² ëŠ”ë°
ChatGPTì˜ ë§:
ì¢‹ì•„, ì´ ì˜¤ë¥˜:

psycopg2.errors.InsufficientPrivilege: permission denied for schema keeyong
ëŠ” ëª…í™•í•˜ê²Œ ë§í•˜ë©´ Redshift ë˜ëŠ” PostgreSQLì—ì„œ "keeyong"ì´ë¼ëŠ” ìŠ¤í‚¤ë§ˆì— ì ‘ê·¼í•  ê¶Œí•œì´ ì—†ì–´ì„œ ìƒê¸´ ê²ƒì´ê³ , ê·¸ ìŠ¤í‚¤ë§ˆë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©í•˜ë ¤ëŠ” SQLì´ ì½”ë“œ ì–´ë”˜ê°€ì— ìˆë‹¤ëŠ” ëœ»ì´ì•¼.

âœ… ì—ëŸ¬ ë°œìƒ ì§€ì 
ì—ëŸ¬ ë©”ì‹œì§€ì— ë”°ë¥´ë©´ ì´ ì½”ë“œì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆì–´:

# /opt/airflow/dags/UpdateSymbol.py
cur.execute(f"DROP TABLE IF EXISTS {schema}.{table};")
ì¦‰, schemaë¼ëŠ” ë³€ìˆ˜ê°€ "keeyong" ê°’ì„ ê°–ê³  ìˆì—ˆë˜ ê²ƒ.

ğŸ” í•´ê²° íë¦„
1. UpdateSymbol.py íŒŒì¼ì—ì„œ ì•„ë˜ë¥¼ ê²€ìƒ‰:
schema = ...
ì˜ˆë¥¼ ë“¤ì–´:

schema = "keeyong"
table = "some_table"
ë˜ëŠ” í•¨ìˆ˜ í˜¸ì¶œì—ì„œ:

load(schema="keeyong", table="weather_forecast", ...)
ë˜ëŠ” DAG ë‚´ì—ì„œ:

PythonOperator(
    task_id="load",
    python_callable=load,
    op_kwargs={
        "schema": "keeyong",
        "table": "weather_forecast"
    },
)
ì´ë ‡ê²Œ op_kwargs ë˜ëŠ” ì¸ìì—ì„œ ë„˜ê²¨ë°›ì•˜ì„ ê°€ëŠ¥ì„±ë„ ìˆìŒ.

2. ë³€ìˆ˜ ì „ë‹¬ ê²½ë¡œë¥¼ ì¶”ì 
ì—ëŸ¬ ìŠ¤íƒì„ ë³´ë©´:

File "/opt/airflow/dags/UpdateSymbol.py", line 38, in load
    cur.execute(f"DROP TABLE IF EXISTS {schema}.{table};")
ì¦‰ load() í•¨ìˆ˜ ì•ˆì—ì„œ schema ë³€ìˆ˜ê°€ "keeyong"ì´ê³ , ì´ë¥¼ ì´ìš©í•´ SQL ì‹¤í–‰í•˜ë‹¤ê°€ í„°ì§„ ê±°ì•¼.

ğŸ›  í•´ê²° ë°©ë²•
ë§Œì•½ "keeyong" ìŠ¤í‚¤ë§ˆë¥¼ ì“°ëŠ” ê²Œ ë§ë‹¤ë©´:
â†’ í•´ë‹¹ DB ê³„ì •ì— ê¶Œí•œì„ ë¶€ì—¬í•´ì•¼ í•¨.

GRANT USAGE ON SCHEMA keeyong TO your_user;
GRANT CREATE, SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA keeyong TO your_user;
ì˜ë„í•œ ìŠ¤í‚¤ë§ˆê°€ "keeyong"ì´ ì•„ë‹ˆë¼ë©´:
â†’ UpdateSymbol.pyì—ì„œ schema = ... ê°’ì„ ìˆ˜ì •í•˜ê±°ë‚˜, DAG ì •ì˜ì—ì„œ ë„˜ê¸°ëŠ” ê°’ì„ ë°”ê¿”ì•¼ í•´.

ğŸ” ìš”ì•½
ì›ì¸	ì„¤ëª…
schema = "keeyong"	ì½”ë“œ ì–´ë”˜ê°€ì—ì„œ í•˜ë“œì½”ë”© ë˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ ì§€ì •
ì—ëŸ¬ ìœ„ì¹˜	cur.execute(f"DROP TABLE IF EXISTS {schema}.{table};")
í•´ê²° ë°©ë²•	ê¶Œí•œ ë¶€ì—¬ ë˜ëŠ” ìŠ¤í‚¤ë§ˆ ì´ë¦„ ìˆ˜ì •
ì½”ë“œ íŒŒì¼ ë³´ì—¬ì£¼ë©´ ì •í™•íˆ ì–´ë””ì„œ "keeyong"ì´ ë“¤ì–´ê°€ëŠ”ì§€ ë°”ë¡œ ì§šì–´ì¤„ ìˆ˜ ìˆì–´. ë³´ì—¬ì¤„ë˜?

ë‚˜ì˜ ë§:
airflow uiì—ì„œ codeë¡œ ë“¤ì–´ê°€ì„œ ìˆ˜ì •ê°€ëŠ¥í–¬? ryanp3 ìœ¼ë¡œ ë³€ê²½í•´ì•¼í•´
ChatGPTì˜ ë§:
ê²°ë¡ ë¶€í„° ë§í•˜ë©´:

âŒ Airflow UIì—ì„œëŠ” DAG ì½”ë“œ ì§ì ‘ ìˆ˜ì • ë¶ˆê°€ëŠ¥í•´
Airflow UIì˜ "Code" íƒ­ì€ ì½ê¸° ì „ìš©(READ-ONLY) ë·°ì•¼.
ì½”ë“œë¥¼ ë³´ê³  í™•ì¸í•  ìˆ˜ëŠ” ìˆì§€ë§Œ, ìˆ˜ì •í•˜ê±°ë‚˜ ì €ì¥í•  ìˆ˜ëŠ” ì—†ì–´.

âœ… DAG ì½”ë“œ ìˆ˜ì • ë°©ë²• (ì‹¤ì œ ë°˜ì˜ë˜ëŠ” ë°©ì‹)
AirflowëŠ” DAG íŒŒì¼ì„ íŠ¹ì • í´ë”ì—ì„œ ë¶ˆëŸ¬ì™€ ì‹¤í–‰í•˜ë¯€ë¡œ,
DAGë¥¼ ìˆ˜ì •í•˜ë ¤ë©´ í˜¸ìŠ¤íŠ¸ OSë‚˜ Docker ê³µìœ  ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ ìˆ˜ì •í•´ì•¼ í•´.

ğŸ”§ ì˜ˆì‹œ (ë„ˆì˜ docker-compose ê¸°ì¤€)
volumes:
  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
ì¦‰, í˜¸ìŠ¤íŠ¸ ë¨¸ì‹ ì˜ ./dags ë””ë ‰í† ë¦¬ì— ìˆëŠ” .py íŒŒì¼ì„ ìˆ˜ì •í•˜ë©´ Airflowê°€ ê·¸ê²ƒì„ ìë™ìœ¼ë¡œ ë°˜ì˜í•´.

ğŸ“ ì‹¤ì œ ìˆ˜ì • ì˜ˆ
1. í˜¸ìŠ¤íŠ¸ ë¨¸ì‹ ì—ì„œ UpdateSymbol.py ì—´ê¸°
ì˜ˆ:

schema = "keeyong"  # â† ìˆ˜ì • ì „
schema = "ryanp3"   # â† ìˆ˜ì • í›„
ë˜ëŠ” DAG ì¸ìì—ì„œ:

op_kwargs={
    "schema": "keeyong"  # â†’ "ryanp3"ë¡œ ìˆ˜ì •
}
2. ì €ì¥ í›„ 5~10ì´ˆ ê¸°ë‹¤ë¦¬ë©´ Airflowê°€ ìë™ ë°˜ì˜
AirflowëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 5ì´ˆ~5ë¶„ë§ˆë‹¤ DAG ë””ë ‰í† ë¦¬ë¥¼ ìŠ¤ìº”í•´ì„œ íŒŒì¼ì„ ë‹¤ì‹œ ì½ìŒ

ìŠ¤ìº” ì£¼ê¸°ëŠ” dag_dir_list_interval (ê¸°ë³¸ 300ì´ˆ) ì„¤ì •ì— ë”°ë¼ ë‹¤ë¦„

âœ… ìˆ˜ì • í›„ í™•ì¸ ë°©ë²•
Airflow UI â†’ DAG í´ë¦­ â†’ â€œCodeâ€ íƒ­ì—ì„œ ìˆ˜ì • ë‚´ìš© ë°˜ì˜ëëŠ”ì§€ í™•ì¸

DAG í…ŒìŠ¤íŠ¸ ë˜ëŠ” Triggerë¡œ ì •ìƒ ì‘ë™ ì—¬ë¶€ ì²´í¬

ğŸ” ìš”ì•½
í•­ëª©	ê°€ëŠ¥ ì—¬ë¶€
Airflow UIì—ì„œ ì½”ë“œ ìˆ˜ì •	âŒ ë¶ˆê°€ëŠ¥ (ì½ê¸° ì „ìš©)
DAG ìˆ˜ì • ìœ„ì¹˜	í˜¸ìŠ¤íŠ¸ ë¨¸ì‹ ì˜ dags/ ë””ë ‰í† ë¦¬ ë‚´ .py íŒŒì¼
ìˆ˜ì • ë°˜ì˜ ì‹œì 	ì €ì¥ í›„ ìë™ ì¸ì‹ (ë³´í†µ 5ì´ˆ~5ë¶„)
ë°˜ì˜ í™•ì¸ ë°©ë²•	UI Code íƒ­ì—ì„œ í™•ì¸ or DAG ì‹¤í–‰
í•„ìš”í•˜ë©´ schema ê°’ì„ ì™¸ë¶€ ì„¤ì •(ì˜ˆ: Variable)ë¡œ ë¹¼ëŠ” ë°©ë²•ë„ ì•Œë ¤ì¤„ ìˆ˜ ìˆì–´.
ì§€ê¸ˆì€ í•˜ë“œì½”ë”©ìœ¼ë¡œ ë˜ì–´ ìˆìœ¼ë‹ˆ, ë³€ìˆ˜ ê´€ë¦¬ê°€ ë” í¸í•´ì§ˆ ìˆ˜ ìˆì–´.

ë‚˜ì˜ ë§:
'gspread'ë¼ëŠ” ëª¨ë“ˆì´ ì—†ë‹¤ëŠ” ì˜¤ë¥˜ê°€ë– 
ChatGPTì˜ ë§:
ì¢‹ì•„, ê·¸ ì—ëŸ¬ëŠ” ì •í™•íˆ ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì´ì•¼:

âŒ ModuleNotFoundError: No module named 'gspread'
ğŸ” ì›ì¸
gspreadëŠ” Google Sheets APIì— ì ‘ê·¼í•˜ê¸° ìœ„í•œ íŒŒì´ì¬ í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ë°,
Airflow ì»¨í…Œì´ë„ˆì— ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ì„œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ì•¼.

âœ… í•´ê²° ë°©ë²•
ë°©ë²• 1. ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© (ë¹ ë¥¸ ì„¤ì¹˜)
ë„ì»¤ ì»¨í…Œì´ë„ˆ ì•ˆì— ì§ì ‘ ë“¤ì–´ê°€ì„œ ì„¤ì¹˜:

docker exec -it <webserver-container-name> bash
pip install gspread
ë‹¨ì : ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘í•˜ë©´ ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ëŠ” ì‚¬ë¼ì§

ë°©ë²• 2. ì˜êµ¬ ë°˜ì˜ (ì¶”ì²œ)
â‘  requirements.txt íŒŒì¼ì— ì¶”ê°€:
gspread
oauth2client  # gspreadì—ì„œ ì¸ì¦ ì‹œ í•¨ê»˜ í•„ìš”
â‘¡ .envì— ì•„ë˜ì²˜ëŸ¼ ì‘ì„± (ê°€ë³ê²Œ í…ŒìŠ¤íŠ¸ìš©)
_PIP_ADDITIONAL_REQUIREMENTS=gspread oauth2client
ì´ë¯¸ .envê°€ ìˆë‹¤ë©´ ê±°ê¸°ì— ì´ì–´ì„œ ë„£ìœ¼ë©´ ë¨

â‘¢ ë˜ëŠ” docker-compose.yaml ìˆ˜ì • (ì§€ì† ìš´ì˜ ëª©ì )
environment:
  _PIP_ADDITIONAL_REQUIREMENTS: gspread oauth2client
ì´ í™˜ê²½ë³€ìˆ˜ëŠ” ì´ë¯¸ êµ¬ì„±ë¼ ìˆìœ¼ë‹ˆ ìœ„ì²˜ëŸ¼ ë‚´ìš©ë§Œ ì¶”ê°€í•˜ë©´ ë¨

â‘£ ë³€ê²½ í›„ ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker-compose down
docker-compose up -d
ğŸ“ ìš”ì•½
ë°©ë²•	ì„¤ëª…	ë¹„ê³ 
ì»¨í…Œì´ë„ˆ ë‚´ pip install gspread	ë¹ ë¥¸ í…ŒìŠ¤íŠ¸	ì¬ì‹œì‘ ì‹œ ì‚¬ë¼ì§
requirements.txt + docker-compose ì¬ë¹Œë“œ	ìš´ì˜ìš©	ì¶”ì²œ ë°©ì‹
_PIP_ADDITIONAL_REQUIREMENTS ì‚¬ìš©	ê°„í¸	ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ìš©, ì¬ì‹œì‘ë§ˆë‹¤ ì„¤ì¹˜ë¨
ì›í•˜ë©´ .env, docker-compose.yaml, Dockerfileì„ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì“¸ì§€ ë‚´ê°€ ì •ë¦¬í•´ì„œ ë§Œë“¤ì–´ì¤„ ìˆ˜ ìˆì–´.
í•„ìš”í•œ ë°©ì‹ ë§í•´ì¤˜.

ë‚˜ì˜ ë§:
2025-05-26T08:14:06.104+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/Weather_to_Redshift_v2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/Weather_to_Redshift_v2.py", line 98, in <module>
    etl("keeyong", "weather_forecast_v2", 37.5665, 126.9780, Variable.get("open_weather_api_key"))
                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable open_weather_api_key does not exist'
ì´ê±´ ë¬´ìŠ¨ ì˜¤ë¥˜ì•¼?
airflow uiì—ì„œ ì¶”ê°€ë¥¼í–ˆëŠ”ë°?
ChatGPTì˜ ë§:
ì˜¤ë¥˜ ë©”ì‹œì§€:

KeyError: 'Variable open_weather_api_key does not exist'
ëŠ” ëª…í™•í•˜ê²Œ Airflowê°€ open_weather_api_keyë¼ëŠ” Variableì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•´. ê·¸ëŸ°ë° ë„ˆëŠ” "UIì—ì„œ ì¶”ê°€í–ˆëŠ”ë° ì™œ ì•ˆ ë˜ì§€?"ë¼ëŠ” ì˜ë¬¸ì´ ìˆëŠ” ê±°ì§€.

ğŸ” ê°€ëŠ¥í•œ ì›ì¸ ì •ë¦¬
âœ… 1. ë³€ìˆ˜ ì´ë¦„ì´ ì •í™•í•˜ì§€ ì•ŠìŒ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„)
Airflowì˜ Variable.get("...")ëŠ” ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•¨.
ì˜ˆë¥¼ ë“¤ì–´, UIì—ì„œ Open_Weather_API_Keyë¡œ ë“±ë¡í–ˆëŠ”ë° ì½”ë“œì—ì„œ open_weather_api_keyë¡œ ì¼ë‹¤ë©´ ëª» ì°¾ëŠ”ë‹¤.

UIì—ì„œ ì •í™•íˆ open_weather_api_key ë¡œ ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´.

âœ… 2. Variable Scope ì˜¤í•´: ì›¹ì„œë²„ vs scheduler vs worker
Variableì€ DBì— ì €ì¥ë˜ì§€ë§Œ,

ì›¹ì„œë²„ì—ì„œ ì¶”ê°€í•œ Variableì´ ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘ ì—†ì´ ë°˜ì˜ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆìŒ
(íŠ¹íˆ Schedulerë‚˜ Workerê°€ ë³„ë„ë¡œ ë„ì›Œì ¸ ìˆëŠ” Celery í™˜ê²½ì—ì„œ)

í•´ê²°: ì›¹ì„œë²„, scheduler, worker ëª¨ë‘ ì¬ì‹œì‘

docker-compose restart
âœ… 3. Variableì´ Airflow ì‹¤í–‰ ì „ì— ì°¸ì¡°ë˜ê³  ìˆìŒ
Weather_to_Redshift_v2.py ì½”ë“œì—ì„œ Variable.get(...)ì„ ì „ì—­(global scope) ì—ì„œ í˜¸ì¶œí•˜ê³  ìˆìŒ:

etl("keeyong", "weather_forecast_v2", 37.5665, 126.9780, Variable.get("open_weather_api_key"))
ì´ê±´ DAG ë¡œë”© ì‹œì ì— ì‹¤í–‰ë˜ëŠ”ë°, ì´ë•Œ Variableì´ ì•„ì§ DBì—ì„œ ë¶ˆëŸ¬ì™€ì§€ì§€ ì•Šì€ ê²½ìš°ì—ë„ KeyErrorê°€ ë‚  ìˆ˜ ìˆìŒ.

âœ… í•´ê²°ë²•: Variable.get(...)ì„ í•¨ìˆ˜ ë‚´ë¶€ë‚˜ PythonOperator ì•ˆì—ì„œ í˜¸ì¶œí•˜ê²Œ ìˆ˜ì •

ğŸ›  ì¶”ì²œ ì½”ë“œ ìˆ˜ì • ì˜ˆ
# BEFORE (ì˜ëª»ëœ ë°©ì‹)
api_key = Variable.get("open_weather_api_key")
etl("keeyong", "weather_forecast_v2", ..., api_key)

# AFTER (ê¶Œì¥ ë°©ì‹)
def call_etl(**context):
    api_key = Variable.get("open_weather_api_key")
    etl("keeyong", "weather_forecast_v2", ..., api_key)

PythonOperator(
    task_id="call_etl",
    python_callable=call_etl,
    provide_context=True,
)
âœ… ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸
í•­ëª©	í™•ì¸
Variable ì´ë¦„ ì •í™•íˆ open_weather_api_keyì¸ê°€?	âœ…
Variable í˜¸ì¶œì´ DAG ë¡œë”© ì‹œì ì´ ì•„ë‹Œ í•¨ìˆ˜ ì•ˆì—ì„œ ì´ë£¨ì–´ì§€ëŠ”ê°€?	âœ…
ì›¹ì„œë²„/ìŠ¤ì¼€ì¤„ëŸ¬/ì›Œì»¤ ëª¨ë‘ ì¬ì‹œì‘í–ˆëŠ”ê°€?	âœ…
Variable ê°’ì„ CLIë¡œ ì§ì ‘ í™•ì¸í•´ë´„?	airflow variables get open_weather_api_key
í•„ìš”í•˜ë©´ ì½”ë“œë¥¼ í•¨ê»˜ ìˆ˜ì •í•´ì¤„ ìˆ˜ ìˆì–´. Weather_to_Redshift_v2.py ë³´ì—¬ì¤„ë˜?

ë‚˜ì˜ ë§:
from airflow import DAG
from airflow.decorators import task
from airflow.models import Variable
from airflow.providers.postgres.hooks.postgres import PostgresHook

from datetime import datetime
from datetime import timedelta

import requests
import logging
import json


def get_Redshift_connection():
    # autocommit is False by default
    hook = PostgresHook(postgres_conn_id='redshift_dev_db')
    return hook.get_conn().cursor()


@task
def etl(schema, table, lat, lon, api_key):

    # https://openweathermap.org/api/one-call-api
    url = f"https://api.openweathermap.org/data/2.5/onecall?lat={lat}&lon={lon}&appid={api_key}&units=metric&exclude=current,minutely,hourly,alerts"
    response = requests.get(url)
    data = json.loads(response.text)

    """
    {'dt': 1622948400, 'sunrise': 1622923873, 'sunset': 1622976631, 'moonrise': 1622915520, 'moonset': 1622962620, 'moon_phase': 0.87, 'temp': {'day': 26.59, 'min': 15.67, 'max': 28.11, 'night': 22.68, 'eve': 26.29, 'morn': 15.67}, 'feels_like': {'day': 26.59, 'night': 22.2, 'eve': 26.29, 'morn': 15.36}, 'pressure': 1003, 'humidity': 30, 'dew_point': 7.56, 'wind_speed': 4.05, 'wind_deg': 250, 'wind_gust': 9.2, 'weather': [{'id': 802, 'main': 'Clouds', 'description': 'scattered clouds', 'icon': '03d'}], 'clouds': 44, 'pop': 0, 'uvi': 3}
    """
    ret = []
    for d in data["daily"]:
        day = datetime.fromtimestamp(d["dt"]).strftime('%Y-%m-%d')
        ret.append("('{}',{},{},{})".format(day, d["temp"]["day"], d["temp"]["min"], d["temp"]["max"]))

    cur = get_Redshift_connection()
    
    # ì›ë³¸ í…Œì´ë¸”ì´ ì—†ë‹¤ë©´ ìƒì„±
    create_table_sql = f"""CREATE TABLE IF NOT EXISTS {schema}.{table} (
    date date,
    temp float,
    min_temp float,
    max_temp float,
    created_date timestamp default GETDATE()
);"""
    logging.info(create_table_sql)

    # ì„ì‹œ í…Œì´ë¸” ìƒì„±
    create_t_sql = f"""CREATE TEMP TABLE t AS SELECT * FROM {schema}.{table};"""
    logging.info(create_t_sql)
    try:
        cur.execute(create_table_sql)
        cur.execute(create_t_sql)
        cur.execute("COMMIT;")
    except Exception as e:
        cur.execute("ROLLBACK;")
        raise

    # ì„ì‹œ í…Œì´ë¸” ë°ì´í„° ì…ë ¥
    insert_sql = f"INSERT INTO t VALUES " + ",".join(ret)
    logging.info(insert_sql)
    try:
        cur.execute(insert_sql)
        cur.execute("COMMIT;")
    except Exception as e:
        cur.execute("ROLLBACK;")
        raise

    # ê¸°ì¡´ í…Œì´ë¸” ëŒ€ì²´
    alter_sql = f"""DELETE FROM {schema}.{table};
      INSERT INTO {schema}.{table}
      SELECT date, temp, min_temp, max_temp FROM (
        SELECT *, ROW_NUMBER() OVER (PARTITION BY date ORDER BY created_date DESC) seq
        FROM t
      )
      WHERE seq = 1;"""
    logging.info(alter_sql)
    try:
        cur.execute(alter_sql)
        cur.execute("COMMIT;")
    except Exception as e:
        cur.execute("ROLLBACK;")
        raise


with DAG(
    dag_id = 'Weather_to_Redshift_v2',
    start_date = datetime(2022,8,24), # ë‚ ì§œê°€ ë¯¸ë˜ì¸ ê²½ìš° ì‹¤í–‰ì´ ì•ˆë¨
    schedule = '0 4 * * *',  # ì ë‹¹íˆ ì¡°ì ˆ
    max_active_runs = 1,
    catchup = False,
    default_args = {
        'retries': 1,
        'retry_delay': timedelta(minutes=3),
    }
) as dag:

    etl("ryanp3", "weather_forecast_v2", 37.5665, 126.9780, Variable.get("open_weather_api_key"))
ChatGPTì˜ ë§:
ì½”ë“œ ì˜ ê°€ì ¸ì™”ì–´. ë¬¸ì œëŠ” ë°”ë¡œ ì´ ë¶€ë¶„ì´ì•¼:

etl("ryanp3", "weather_forecast_v2", 37.5665, 126.9780, Variable.get("open_weather_api_key"))
âŒ ë¬¸ì œì 
ì´ ì½”ë“œëŠ” DAG ì •ì˜ì˜ ì „ì—­ ìŠ¤ì½”í”„(global scope) ì—ì„œ Variable.get(...)ì„ í˜¸ì¶œí•˜ê³  ìˆì–´ì„œ, DAGì„ Airflowê°€ ë¡œë”©í•  ë•Œë§ˆë‹¤ ì‹¤í–‰í•˜ë ¤ê³  í•˜ê²Œ ë¨.

ê·¸ëŸ°ë° ì´ ì‹œì ì—ì„œëŠ” Variableì´ ì•„ì§ ë©”ëª¨ë¦¬ì— ì˜¬ë¼ì˜¤ê¸° ì „ì¼ ìˆ˜ë„ ìˆê³ , UIì—ì„œ ì¶”ê°€í–ˆë”ë¼ë„ schedulerë‚˜ webserverê°€ ì¬ì‹œì‘ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë°˜ì˜ë˜ì§€ ì•ŠìŒ.
ê·¸ ê²°ê³¼ë¡œ KeyErrorê°€ ë°œìƒí•˜ëŠ” ê±°ì•¼.

âœ… í•´ê²° ë°©ë²•
Variable.get(...)ì„ DAGì´ ì‹¤ì œ ì‹¤í–‰ë  ë•Œ ë¶ˆëŸ¬ì˜¤ë„ë¡ ë°”ê¿”ì•¼ í•´.

ğŸ”§ ìˆ˜ì • ì½”ë“œ
@task
def etl_task():
    schema = "ryanp3"
    table = "weather_forecast_v2"
    lat, lon = 37.5665, 126.9780
    api_key = Variable.get("open_weather_api_key")  # ì´ì œ ì—¬ê¸°ì„œ ì•ˆì „í•˜ê²Œ ë¶ˆëŸ¬ì˜´

    etl(schema, table, lat, lon, api_key)

with DAG(
    dag_id='Weather_to_Redshift_v2',
    start_date=datetime(2022, 8, 24),
    schedule='0 4 * * *',
    max_active_runs=1,
    catchup=False,
    default_args={
        'retries': 1,
        'retry_delay': timedelta(minutes=3),
    }
) as dag:
    etl_task()
ğŸ“ ìš”ì•½
ë¬¸ì œ	í•´ê²°
DAG íŒŒì¼ ë¡œë”© ì‹œì ì— Variable.get(...) ì‹¤í–‰ë¨	@task ë‚´ë¶€ë¡œ ì˜®ê²¨ì„œ DAG ì‹¤í–‰ ì‹œì ì—ë§Œ ì‹¤í–‰ë˜ê²Œ
Variable UIì—ì„œ ì¶”ê°€í–ˆì§€ë§Œ ëª» ì°¾ìŒ	Airflowê°€ Variableì„ DAG ë¡œë”© ì „ì— ë¶ˆëŸ¬ì˜¤ë ¤ í•˜ê¸° ë•Œë¬¸
ì´ë ‡ê²Œ ë°”ê¾¸ë©´ Variableì´ ì•ˆì „í•˜ê²Œ ë¡œë”©ë˜ê³ , UIì—ì„œ ì„¤ì •í•œ ê°’ë„ ì˜ ì‘ë™í•  ê±°ì•¼.
í…ŒìŠ¤íŠ¸ í›„ ë¬¸ì œ ìƒê¸°ë©´ ì•Œë ¤ì¤˜.

ë‚˜ì˜ ë§:
Broken DAG: [/opt/airflow/dags/Weather_to_Redshift_v2.py]
Traceback (most recent call last):
  File "/opt/airflow/dags/Weather_to_Redshift_v2.py", line 98, in <module>
    etl("ryanp3", "weather_forecast_v2", 37.5665, 126.9780, Variable.get("open_weather_api_key"))
                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable open_weather_api_key does not exist'
ì´ê±´ ë¬´ìŠ¨ ì˜¤ë¥˜ì•¼
ChatGPTì˜ ë§:
ì´ê±´ ì•„ì§ ì½”ë“œ ìˆ˜ì •ì´ ì•ˆ ëœ ìƒíƒœì—ì„œ ë°œìƒí•˜ëŠ” ë™ì¼í•œ ì˜¤ë¥˜ì•¼.

ğŸ”¥ ì˜¤ë¥˜ í•µì‹¬
etl("ryanp3", "weather_forecast_v2", 37.5665, 126.9780, Variable.get("open_weather_api_key"))
ì´ ì¤„ì´ DAG íŒŒì¼ì˜ ì „ì—­ ìŠ¤ì½”í”„ì— ìˆê¸° ë•Œë¬¸ì—,
Airflowê°€ DAGì„ ë¡œë“œí•˜ëŠ” ìˆœê°„ Variable.get(...)ì´ ì‹¤í–‰ë˜ë©´ì„œ ë³€ìˆ˜ ì—†ë‹¤ê³  ì—ëŸ¬ ë‚˜ëŠ” ê±°ì•¼.

AirflowëŠ” DAG íŒŒì¼ì„ ìŠ¤ìº”í•  ë•Œ Variable.get() í˜¸ì¶œì„ í—ˆìš©í•˜ì§€ ì•ŠìŒ (ìŠ¤ì¼€ì¤„ë§ ì´ì „ ë‹¨ê³„ì´ê¸° ë•Œë¬¸)

âœ… í•´ê²° ë°©ë²•: ë°˜ë“œì‹œ ì´ êµ¬ì¡°ë¡œ ë°”ê¿”
ì•„ë˜ì²˜ëŸ¼ @task ë°ì½”ë ˆì´í„° í•¨ìˆ˜ ì•ˆì—ì„œ Variable.get(...)ì„ í˜¸ì¶œí•˜ë„ë¡ ë³€ê²½í•´ì•¼ í•´.
ì¤‘ìš”: ì „ì—­ì—ì„œ í˜¸ì¶œí•˜ë©´ ì ˆëŒ€ ì•ˆ ë¨

âœ… ìˆ˜ì • ì½”ë“œ ì˜ˆì‹œ
@task
def etl_task():
    schema = "ryanp3"
    table = "weather_forecast_v2"
    lat, lon = 37.5665, 126.9780
    api_key = Variable.get("open_weather_api_key")  # ì—¬ê¸°ì„œë§Œ í˜¸ì¶œ!

    etl(schema, table, lat, lon, api_key)

with DAG(
    dag_id='Weather_to_Redshift_v2',
    start_date=datetime(2022, 8, 24),
    schedule='0 4 * * *',
    max_active_runs=1,
    catchup=False,
    default_args={
        'retries': 1,
        'retry_delay': timedelta(minutes=3),
    }
) as dag:
    etl_task()
ğŸ” í•œ ì¤„ ìš”ì•½
Variable.get(...)ì€ @task ì•ˆì—ì„œë§Œ ì¨ë¼. ì „ì—­ì— ì“°ë©´ DAG ë¡œë”© ë‹¨ê³„ì—ì„œ í„°ì§„ë‹¤.

âœ”ï¸ ì¶”ê°€ í™•ì¸ì‚¬í•­
Airflow UI > Admin > Variables ì— open_weather_api_keyê°€ ì •í™•íˆ ì¡´ì¬í•´ì•¼ í•˜ê³ 

ì •í™•íˆ ì´ ë¬¸ìì—´: open_weather_api_key (ëŒ€ì†Œë¬¸ìê¹Œì§€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨)

ì´ì œ DAG ì •ìƒ ë¡œë”©ë  ê±°ì•¼.
ì½”ë“œë¥¼ ìˆ˜ì •í•˜ê³  ì €ì¥í•˜ë©´ UIì—ì„œ Broken DAG ì—ëŸ¬ê°€ ì‚¬ë¼ì§ˆ ê±°ì•¼.

ë‚˜ì˜ ë§:
 Found local files:
***   * /opt/airflow/logs/dag_id=UpdateSymbol_v2/run_id=manual__2025-05-26T08:32:54.420165+00:00/task_id=get_historical_prices/attempt=1.log.SchedulerJob.log
[2025-05-26, 08:32:57 UTC] {task_context_logger.py:104} ERROR - Executor reports task instance <TaskInstance: UpdateSymbol_v2.get_historical_prices manual__2025-05-26T08:32:54.420165+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
ì´ê±´ ë¬´ìŠ¨ì˜¤ë¥˜
ChatGPTì˜ ë§:
ì´ ì—ëŸ¬ ë©”ì‹œì§€ëŠ” Airflowê°€ ì•„ë˜ ë‘ ìƒíƒœ ì‚¬ì´ì˜ ë¶ˆì¼ì¹˜ë¥¼ ê°ì§€í–ˆì„ ë•Œ ë°œìƒí•˜ëŠ” ëŒ€í‘œì ì¸ í˜„ìƒì´ì•¼:

Executor reports task instance <...> finished (failed) although the task says it's queued.
ğŸ“Œ ì˜ë¯¸
Airflowê°€ ë‚´ë¶€ì ìœ¼ë¡œëŠ” í•´ë‹¹ TaskInstanceê°€ "queued" ìƒíƒœë¼ê³  ì•Œê³  ìˆëŠ”ë°,
Executor (ì˜ˆ: CeleryExecutor, LocalExecutor ë“±) ì¸¡ì—ì„œëŠ” ì´ë¯¸ ì‹¤íŒ¨(failed)ë¡œ ì¢…ë£Œë˜ì—ˆë‹¤ê³  ë³´ê³ í•˜ê³  ìˆë‹¤ëŠ” ëœ»ì´ì•¼.

ì¦‰, Airflowì˜ DB ìƒíƒœì™€ Executorì˜ ì‹¤ì œ ì‹¤í–‰ ê²°ê³¼ê°€ ë¶ˆì¼ì¹˜í•´ì„œ ì¶©ëŒ ë‚œ ê²ƒ.

ğŸ” ì›ì¸ ì •ë¦¬
ì›ì¸	ì„¤ëª…
âŒ ì™¸ë¶€ì—ì„œ task í”„ë¡œì„¸ìŠ¤ë¥¼ ê°•ì œë¡œ kill	ì˜ˆ: ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘, OOM kill ë“±
âŒ ì‘ì—… ë„ì¤‘ ì»¨í…Œì´ë„ˆ ìì²´ê°€ down ë¨	docker-compose restart, Docker Desktop ë„ê¸° ë“±
âŒ ë¡œê·¸/ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ê¼¬ì„	airflow schedulerê°€ ì¤‘ê°„ì— êº¼ì¡Œê±°ë‚˜ ì£½ìŒ
âŒ ë¡œê·¸ ê²½ë¡œ ì˜¤ë¥˜ë‚˜ ê¶Œí•œ ë¬¸ì œ	ë¡œê·¸ íŒŒì¼ì„ ì“¸ ìˆ˜ ì—†ê±°ë‚˜, íŒŒì¼ì´ ì‚­ì œë¨
âŒ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë¬¸ì œ	ë©”ëª¨ë¦¬ ë¶€ì¡±, CPU í¬í™”ë¡œ workerê°€ ë¹„ì •ìƒ ì¢…ë£Œ
âœ… í•´ê²° ë°©ë²•
1. ë‹¤ì‹œ DAG Run ì‹¤í–‰
UIì—ì„œ í•´ë‹¹ DAG Runì„ ì¤‘ì§€ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ ë³´ê±°ë‚˜, CLIë¡œ ë‹¤ì‹œ í…ŒìŠ¤íŠ¸:

airflow dags test UpdateSymbol_v2 2025-05-26
2. ìŠ¤ì¼€ì¤„ëŸ¬ì™€ ì›Œì»¤ ìƒíƒœ í™•ì¸
docker-compose ps
CeleryExecutor ê¸°ë°˜ì´ë©´:

docker-compose logs airflow-scheduler
docker-compose logs airflow-worker
í˜¹ì‹œ ì›Œì»¤ë‚˜ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì£½ì—ˆëŠ”ì§€ í™•ì¸í•´.

3. ì „ì²´ ì¬ì‹œì‘ (ë³´í†µ í•´ê²°ë¨)
docker-compose down
docker-compose up -d
íŠ¹íˆ ê°œë°œìš© í™˜ê²½ì—ì„œ ì—¬ëŸ¬ DAGì´ ê¼¬ì˜€ì„ ë• ì´ê²Œ ê°€ì¥ ê¹”ë”í•˜ê²Œ í•´ê²°ë¨.

ğŸ”’ ì¶”ê°€ íŒ: ë¡œê·¸ ì†ìƒ ë°©ì§€
AirflowëŠ” task logë¥¼ ì•„ë˜ ìœ„ì¹˜ì— ì €ì¥í•¨:

/opt/airflow/logs/dag_id=.../task_id=.../attempt=1.log
ì´ ê²½ë¡œê°€ Docker ì»¨í…Œì´ë„ˆì—ì„œ í˜¸ìŠ¤íŠ¸ì— ë§ˆìš´íŠ¸ëœ ë¡œê·¸ ë””ë ‰í† ë¦¬ì™€ ì˜ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ volumes: ì„¤ì • í™•ì¸ë„ ì¤‘ìš”í•´.

ğŸ“ ìš”ì•½
ë¬¸ì œ	í•´ê²°
ExecutorëŠ” ì‹¤íŒ¨ë¡œ ì²˜ë¦¬í–ˆëŠ”ë° DB ìƒíƒœëŠ” queued	ë¡œê·¸ ì†ìƒ, í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œ, ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ë‹¨ ë“±
í•´ê²° 1	DAG ì¬ì‹¤í–‰ (airflow dags test ...)
í•´ê²° 2	docker-compose restart or ì „ì²´ ì¬ì‹œì‘
í•´ê²° 3	ìŠ¤ì¼€ì¤„ëŸ¬/ì›Œì»¤ê°€ ì‚´ì•„ ìˆëŠ”ì§€ ps + logsë¡œ í™•ì¸
í•„ìš”í•˜ë©´ í˜„ì¬ docker-compose ps ê²°ê³¼ë‚˜ ë¡œê·¸ë¥¼ í™•ì¸í•´ì¤„ ìˆ˜ ìˆì–´. ë¶™ì—¬ì¤˜.

from airflow import DAG
from airflow.decorators import task
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta
import requests
import json
import logging


def get_Redshift_cursor():
    hook = PostgresHook(postgres_conn_id='redshift_dev_db')
    return hook.get_conn().cursor()


@task
def fetch_country_data():
    url = "https://restcountries.com/v3/all"
    response = requests.get(url)
    response.raise_for_status()
    return response.json()


@task
def load_to_redshift(data):
    schema = "ryanp3"
    table = "country_info"

    rows = []
    for country in data:
        try:
            name = country["name"]["official"].replace("'", "''")
            population = country.get("population", 0)
            area = country.get("area", 0.0)
            rows.append(f"('{name}', {population}, {area})")
        except KeyError:
            continue

    if not rows:
        raise ValueError("No valid data to insert.")

    cur = get_Redshift_cursor()

    create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {schema}.{table} (
            name VARCHAR(256),
            population BIGINT,
            area FLOAT
        );
    """

    delete_sql = f"DELETE FROM {schema}.{table};"
    insert_sql = f"INSERT INTO {schema}.{table} (name, population, area) VALUES " + ",".join(rows)

    try:
        logging.info("Creating table if not exists...")
        cur.execute(create_table_sql)
        logging.info("Clearing existing data...")
        cur.execute(delete_sql)
        logging.info("Inserting new data...")
        cur.execute(insert_sql)
        cur.execute("COMMIT;")
    except Exception as e:
        cur.execute("ROLLBACK;")
        logging.error(f"Redshift insertion failed: {e}")
        raise


with DAG(
    dag_id='country_info_to_redshift',
    start_date=datetime(2024, 1, 1),
    schedule='30 6 * * 6',  # ë§¤ì£¼ í† ìš”ì¼ 06:30 UTC
    catchup=False,
    max_active_runs=1,
    default_args={
        'retries': 1,
        'retry_delay': timedelta(minutes=3),
    },
    tags=['country', 'api', 'redshift'],
) as dag:
    raw_data = fetch_country_data()
    load_to_redshift(raw_data)
